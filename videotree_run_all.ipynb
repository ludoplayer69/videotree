{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:01:51.880581Z",
     "iopub.status.busy": "2025-08-09T18:01:51.880364Z",
     "iopub.status.idle": "2025-08-09T18:01:52.980016Z",
     "shell.execute_reply": "2025-08-09T18:01:52.979083Z",
     "shell.execute_reply.started": "2025-08-09T18:01:51.880560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'videotree'...\n",
      "remote: Enumerating objects: 12, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
      "remote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (12/12), 1.35 MiB | 5.15 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ludoplayer69/videotree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-09T18:01:52.981250Z",
     "iopub.status.busy": "2025-08-09T18:01:52.980964Z",
     "iopub.status.idle": "2025-08-09T18:03:25.457448Z",
     "shell.execute_reply": "2025-08-09T18:03:25.456722Z",
     "shell.execute_reply.started": "2025-08-09T18:01:52.981209Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.33.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, nvidia-cusolver-cu12, transformers, bitsandbytes, accelerate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.33.1\n",
      "    Uninstalling huggingface-hub-0.33.1:\n",
      "      Successfully uninstalled huggingface-hub-0.33.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.10.0 bitsandbytes-0.46.1 huggingface_hub-0.34.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.55.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes accelerate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-08-09T18:05:04.291113Z",
     "iopub.status.busy": "2025-08-09T18:05:04.290343Z",
     "iopub.status.idle": "2025-08-09T18:05:07.689422Z",
     "shell.execute_reply": "2025-08-09T18:05:07.688749Z",
     "shell.execute_reply.started": "2025-08-09T18:05:04.291084Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Videos:   0%|          | 0/2 [00:00<?, ?it/s]Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZdZ8aUcBNzndj135bqFrxb9L816EMGp1\n",
      "To: /kaggle/working/downloaded_videos/0074f737-11cb-497d-8d07-77c3a8127391.mp4\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.5M/15.5M [00:00<00:00, 221MB/s]\n",
      "Downloading Videos:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.74s/it]Downloading...\n",
      "From: https://drive.google.com/uc?id=1bVNvPX6BNPIqcqMk-ZJr6WLsXNQybg64\n",
      "To: /kaggle/working/downloaded_videos/00b9a0de-c59e-49cb-a127-6081e2fb8c8e.mp4\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.2M/12.2M [00:00<00:00, 208MB/s]\n",
      "Downloading Videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gdown  # pip install gdown\n",
    "\n",
    "DRIVE_JSON_PATH = Path('/kaggle/working/videotree/drive_ids.json')  # Update with your actual path\n",
    "VIDEO_SAVE_PATH = Path('/kaggle/working/downloaded_videos')\n",
    "MAX_VIDEOS = 2  # Set your preferred limit\n",
    "\n",
    "VIDEO_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def download_videos(drive_json, max_downloads):\n",
    "    count = 0\n",
    "    pbar = tqdm(total=min(len(drive_json), max_downloads), desc=\"Downloading Videos\")\n",
    "\n",
    "    for uuid, drive_id in drive_json.items():\n",
    "        if count >= max_downloads:\n",
    "            break\n",
    "\n",
    "        output_file = VIDEO_SAVE_PATH / f\"{uuid}.mp4\"\n",
    "        gdown.download(id=drive_id, output=str(output_file), quiet=False)\n",
    "        count += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    drive_data = load_json(DRIVE_JSON_PATH)\n",
    "    download_videos(drive_data, MAX_VIDEOS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:05:10.721393Z",
     "iopub.status.busy": "2025-08-09T18:05:10.720727Z",
     "iopub.status.idle": "2025-08-09T18:05:29.469711Z",
     "shell.execute_reply": "2025-08-09T18:05:29.469089Z",
     "shell.execute_reply.started": "2025-08-09T18:05:10.721370Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys for loading model: []\n",
      "Unexpected keys for loading model: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.44s/it]\n",
      "Extracting features:   0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_36/3803968643.py:125: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():\n",
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "# ---- Globals ----\n",
    "import os, sys, json, cv2, torch\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "INPUT_VIDEOS = Path('/kaggle/working/downloaded_videos')\n",
    "FRAMES_DIR   = Path('/kaggle/working/extracted_frames')\n",
    "FEATURES_DIR = Path('/kaggle/working/extracted_features')\n",
    "ANNOTATION_PATH = Path('/kaggle/input/fullset_anno.json')\n",
    "\n",
    "FRAMES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Params\n",
    "FPS = 1\n",
    "MAX_EXAMPLES = 50\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global model state\n",
    "perceptionclip_model = None\n",
    "perceptionclip_preprocess = None\n",
    "\n",
    "# ---- Utilities ----\n",
    "def load_json(file_path: Path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_image_features(img_feats: torch.Tensor, name_id: str, save_folder: Path):\n",
    "    torch.save(img_feats, save_folder / f\"{name_id}.pt\")\n",
    "\n",
    "def _numeric_sort_key(p: Path):\n",
    "    try:\n",
    "        return int(p.stem)\n",
    "    except ValueError:\n",
    "        return p.stem\n",
    "\n",
    "# ---- Frame extraction ----\n",
    "def extract_frames(videos: list[Path] | None = None, fps: int = FPS):\n",
    "    video_iter = videos if videos is not None else list(INPUT_VIDEOS.iterdir())\n",
    "    for video_fp in tqdm(video_iter, desc=\"Extracting frames\"):\n",
    "        out_dir = FRAMES_DIR / video_fp.stem\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        cap = cv2.VideoCapture(str(video_fp))\n",
    "        if not cap.isOpened():\n",
    "            print(f\"[WARN] Could not open video: {video_fp}\")\n",
    "            continue\n",
    "\n",
    "        fps_ori = cap.get(cv2.CAP_PROP_FPS)\n",
    "        try:\n",
    "            fps_ori = int(fps_ori) if fps_ori and fps_ori > 0 else 1\n",
    "        except Exception:\n",
    "            fps_ori = 1\n",
    "        frame_interval = max(1, int(fps_ori // max(1, fps)))\n",
    "\n",
    "        count = 0\n",
    "        success, img = cap.read()\n",
    "        while success:\n",
    "            if count % frame_interval == 0:\n",
    "                cv2.imwrite(str(out_dir / f\"{count}.jpg\"), img)\n",
    "            success, img = cap.read()\n",
    "            count += 1\n",
    "        cap.release()\n",
    "\n",
    "# ---- Model loading (singleton) ----\n",
    "def ensure_model_loaded(model_name: str = 'PE-Core-B16-224', force_reload: bool = False):\n",
    "    global perceptionclip_model, perceptionclip_preprocess\n",
    "\n",
    "    if perceptionclip_model is not None and perceptionclip_preprocess is not None and not force_reload:\n",
    "        return perceptionclip_model, perceptionclip_preprocess\n",
    "\n",
    "    # One-time setup\n",
    "    if not Path('perception_models').exists():\n",
    "        os.system('git clone https://github.com/facebookresearch/perception_models.git')\n",
    "        os.system('pip install -q decord ftfy')\n",
    "\n",
    "    sys.path.append('./perception_models')\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir('./perception_models')\n",
    "\n",
    "    import core.vision_encoder.pe as pe\n",
    "    import core.vision_encoder.transforms as transforms\n",
    "\n",
    "    model = pe.CLIP.from_config(model_name, pretrained=True).to(DEVICE).eval()\n",
    "    preprocess = transforms.get_image_transform(model.image_size)\n",
    "\n",
    "    os.chdir(cwd)\n",
    "\n",
    "    perceptionclip_model = model\n",
    "    perceptionclip_preprocess = preprocess\n",
    "    return perceptionclip_model, perceptionclip_preprocess\n",
    "\n",
    "# ---- Feature extraction ----\n",
    "@torch.inference_mode()\n",
    "def extract_features_for_dir(example_dir: Path):\n",
    "    if not example_dir.exists():\n",
    "        print(f\"[WARN] Frames dir not found: {example_dir}\")\n",
    "        return\n",
    "\n",
    "    model, preprocess = ensure_model_loaded()\n",
    "\n",
    "    image_files = sorted(list(example_dir.iterdir()), key=_numeric_sort_key)\n",
    "    feats_list = []\n",
    "\n",
    "    # Use autocast only when CUDA is available\n",
    "    # use_cuda = torch.cuda.is_available()\n",
    "    # autocast_ctx = torch.cuda.amp.autocast if use_cuda else torch.autocast\n",
    "    # autocast_kwargs = ({'dtype': torch.float16} if not use_cuda else {})\n",
    "\n",
    "    # with (autocast_ctx(device_type='cuda') if not autocast_kwargs else autocast_ctx(**autocast_kwargs)):\n",
    "    #     for img_fp in image_files:\n",
    "    #         img = Image.open(img_fp).convert('RGB')\n",
    "    #         inp = perceptionclip_preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "    #         feat = perceptionclip_model.encode_image(inp)\n",
    "    #         feats_list.append(feat)\n",
    "\n",
    "    # refactored\n",
    "    for image_file in image_files:\n",
    "\n",
    "        inputs = preprocess(Image.open(image_file)).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            feats = model.encode_image(inputs)\n",
    "            feats_list.append(feats)\n",
    "    \n",
    "        if len(feats_list) == 0:\n",
    "            print(f\"[WARN] No frames in {example_dir}\")\n",
    "            return\n",
    "\n",
    "    stacked = torch.cat(feats_list, dim=0)  # [T, D]\n",
    "    save_image_features(stacked, example_dir.name, FEATURES_DIR)\n",
    "\n",
    "def extract_features_from_all(MAX: int = MAX_EXAMPLES, filter_by_json: Path | None = ANNOTATION_PATH):\n",
    "    valid_names = None\n",
    "    if filter_by_json and Path(filter_by_json).exists():\n",
    "        valid_names = set(load_json(filter_by_json).keys())\n",
    "\n",
    "    dirs = [d for d in FRAMES_DIR.iterdir() if d.is_dir()]\n",
    "    processed = 0\n",
    "    for d in tqdm(dirs, desc=\"Extracting features\"):\n",
    "        if processed >= MAX:\n",
    "            break\n",
    "        if valid_names is not None and d.name not in valid_names:\n",
    "            continue\n",
    "        extract_features_for_dir(d)\n",
    "        processed += 1\n",
    "\n",
    "# ---- Single-video convenience ----\n",
    "def infer_single_video(video_fp: Path):\n",
    "    # 1) Extract frames for just this video\n",
    "    extract_frames(videos=[video_fp], fps=FPS)\n",
    "    # 2) Extract features for the corresponding frames directory\n",
    "    example_dir = FRAMES_DIR / video_fp.stem\n",
    "    extract_features_for_dir(example_dir)\n",
    "\n",
    "# ---- Pipeline entrypoints ----\n",
    "def run_pipeline(process_all: bool = True, video_list: list[Path] | None = None):\n",
    "    \"\"\"\n",
    "    - If process_all=True, process every video under INPUT_VIDEOS.\n",
    "    - If process_all=False, expects video_list (list of Paths) and processes only those.\n",
    "    \"\"\"\n",
    "    ensure_model_loaded()  # loaded once, reused thereafter\n",
    "\n",
    "    # Extract frames\n",
    "    if process_all:\n",
    "        extract_frames()\n",
    "    else:\n",
    "        assert video_list is not None and len(video_list) > 0, \"Provide video_list when process_all=False.\"\n",
    "        extract_frames(videos=video_list)\n",
    "\n",
    "    # Extract features\n",
    "    if process_all:\n",
    "        extract_features_from_all()\n",
    "    else:\n",
    "        for v in video_list:\n",
    "            extract_features_for_dir(FRAMES_DIR / v.stem)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load once and reuse across calls\n",
    "    ensure_model_loaded()\n",
    "    # Example: process everything under INPUT_VIDEOS\n",
    "    run_pipeline(process_all=True)\n",
    "    # Example: infer a single video repeatedly without reloading:\n",
    "    # infer_single_video(Path('/kaggle/working/downloaded_videos/your_uuid.mp4'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:06:59.312047Z",
     "iopub.status.busy": "2025-08-09T18:06:59.311760Z",
     "iopub.status.idle": "2025-08-09T18:06:59.415936Z",
     "shell.execute_reply": "2025-08-09T18:06:59.415157Z",
     "shell.execute_reply.started": "2025-08-09T18:06:59.312029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Found 2 videos.\n",
      "Outputs folder does not exist.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“ Navigate to working directory\n",
    "%cd /kaggle/working/\n",
    "\n",
    "# ğŸ“˜ Load JSON data\n",
    "import json\n",
    "ques_path_json = '/kaggle/input/egoschema/fullset_anno.json'\n",
    "with open(ques_path_json, \"r\") as f:\n",
    "    data = json.load(f)  # Dictionary of video_id â question/options\n",
    "\n",
    "# ğŸ“‚ Get list of video IDs (without .mp4 extension)\n",
    "import os\n",
    "video_folder = \"/kaggle/working/downloaded_videos\"\n",
    "video_ids = [\n",
    "    os.path.splitext(f)[0]\n",
    "    for f in os.listdir(video_folder)\n",
    "    if f.endswith(\".mp4\")\n",
    "]\n",
    "print(f\"Found {len(video_ids)} videos.\")\n",
    "\n",
    "# âœï¸ Generate question prompts for available videos\n",
    "import string\n",
    "prompts = {}\n",
    "\n",
    "for vid in video_ids:\n",
    "    if vid not in data:\n",
    "        continue  # Skip videos without annotation\n",
    "\n",
    "    option_keys = sorted(\n",
    "        [k for k in data[vid] if k.startswith(\"option \")],\n",
    "        key=lambda x: int(x.split()[1])\n",
    "    )\n",
    "\n",
    "    prompt = f\"Question:\\n{data[vid]['question']}\\n\\nOptions:\\n\"\n",
    "    for letter, key in zip(string.ascii_uppercase, option_keys):\n",
    "        prompt += f\"{letter}. {data[vid][key]}\\n\"\n",
    "    prompt += \"\\nPlease choose the most appropriate answer (Aâ€“E).\"\n",
    "\n",
    "    prompts[vid] = prompt\n",
    "\n",
    "# ğŸ“¦ Final dictionary of all questions\n",
    "questions = prompts\n",
    "\n",
    "# ğŸ—‘ï¸ Delete 'outputs' folder if it exists\n",
    "import shutil\n",
    "outputs_folder = \"/kaggle/working/outputs\"\n",
    "if os.path.exists(outputs_folder):\n",
    "    shutil.rmtree(outputs_folder)\n",
    "    print(\"Outputs folder and contents deleted successfully.\")\n",
    "else:\n",
    "    print(\"Outputs folder does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:07:01.809827Z",
     "iopub.status.busy": "2025-08-09T18:07:01.809227Z",
     "iopub.status.idle": "2025-08-09T18:07:02.503475Z",
     "shell.execute_reply": "2025-08-09T18:07:02.502471Z",
     "shell.execute_reply.started": "2025-08-09T18:07:01.809801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'kmeans_pytorch'...\n",
      "remote: Enumerating objects: 422, done.\u001b[K\n",
      "remote: Counting objects: 100% (142/142), done.\u001b[K\n",
      "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
      "remote: Total 422 (delta 64), reused 130 (delta 58), pack-reused 280 (from 1)\u001b[K\n",
      "Receiving objects: 100% (422/422), 1.05 MiB | 22.81 MiB/s, done.\n",
      "Resolving deltas: 100% (184/184), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/subhadarship/kmeans_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:07:02.790375Z",
     "iopub.status.busy": "2025-08-09T18:07:02.789827Z",
     "iopub.status.idle": "2025-08-09T18:07:03.566713Z",
     "shell.execute_reply": "2025-08-09T18:07:03.565989Z",
     "shell.execute_reply.started": "2025-08-09T18:07:02.790344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'VideoTree'...\n",
      "remote: Enumerating objects: 102, done.\u001b[K\n",
      "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
      "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
      "remote: Total 102 (delta 43), reused 53 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (102/102), 3.76 MiB | 27.51 MiB/s, done.\n",
      "Resolving deltas: 100% (43/43), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Ziyang412/VideoTree.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:07:05.254342Z",
     "iopub.status.busy": "2025-08-09T18:07:05.253619Z",
     "iopub.status.idle": "2025-08-09T18:07:08.688023Z",
     "shell.execute_reply": "2025-08-09T18:07:08.687032Z",
     "shell.execute_reply.started": "2025-08-09T18:07:05.254313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.31.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Downloading groq-0.31.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: groq\n",
      "Successfully installed groq-0.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:07:08.689968Z",
     "iopub.status.busy": "2025-08-09T18:07:08.689701Z",
     "iopub.status.idle": "2025-08-09T18:07:08.694265Z",
     "shell.execute_reply": "2025-08-09T18:07:08.693449Z",
     "shell.execute_reply.started": "2025-08-09T18:07:08.689942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:07:08.695445Z",
     "iopub.status.busy": "2025-08-09T18:07:08.695183Z",
     "iopub.status.idle": "2025-08-09T18:07:08.710965Z",
     "shell.execute_reply": "2025-08-09T18:07:08.710260Z",
     "shell.execute_reply.started": "2025-08-09T18:07:08.695423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/VideoTree',\n",
       " '/kaggle/working/kmeans_pytorch',\n",
       " '/kaggle/working/perception_models',\n",
       " '/kaggle/working/extracted_frames',\n",
       " '/kaggle/working/videotree',\n",
       " '/kaggle/working/downloaded_videos',\n",
       " '/kaggle/working/extracted_features']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob('/kaggle/working/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:07:50.805799Z",
     "iopub.status.busy": "2025-08-09T18:07:50.805499Z",
     "iopub.status.idle": "2025-08-09T18:07:51.954165Z",
     "shell.execute_reply": "2025-08-09T18:07:51.953506Z",
     "shell.execute_reply.started": "2025-08-09T18:07:50.805778Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/videotree\n",
      "Cloning into 'VideoTree'...\n",
      "remote: Enumerating objects: 102, done.\u001b[K\n",
      "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
      "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
      "remote: Total 102 (delta 43), reused 53 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (102/102), 3.76 MiB | 30.57 MiB/s, done.\n",
      "Resolving deltas: 100% (43/43), done.\n",
      "/kaggle/working/videotree/VideoTree\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/videotree\n",
    "!git clone https://github.com/Ziyang412/VideoTree.git\n",
    "%cd /kaggle/working/videotree/VideoTree\n",
    "# Import your original modules\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:07:56.253692Z",
     "iopub.status.busy": "2025-08-09T18:07:56.252871Z",
     "iopub.status.idle": "2025-08-09T18:08:06.802328Z",
     "shell.execute_reply": "2025-08-09T18:08:06.801715Z",
     "shell.execute_reply.started": "2025-08-09T18:07:56.253664Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/kmeans_pytorch\n",
      "/kaggle/working\n",
      "Multi-Video Configuration loaded!\n",
      "ğŸ“¹ Videos to process: 2\n",
      "â“ Questions available: 2\n",
      "âœ… All videos have corresponding questions\n",
      "Initializing Groq model...\n",
      "âœ… Groq model initialized: llama-3.1-8b-instant\n",
      "âœ… Utility functions loaded!\n",
      "ğŸš€ STARTING BATCH PROCESSING\n",
      "================================================================================\n",
      "ğŸ“¹ Total videos to process: 2\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ VIDEO 1/2: 0074f737-11cb-497d-8d07-77c3a8127391\n",
      "\n",
      "ğŸ¬ PROCESSING VIDEO: 0074f737-11cb-497d-8d07-77c3a8127391\n",
      "============================================================\n",
      "ğŸ“Š Loading features...\n",
      "âœ… Loaded features: shape torch.Size([180, 1024])\n",
      "ğŸ“ Loading captions...\n",
      "âœ… Loaded 180 frame captions\n",
      "â“ Question: ...\n",
      "ğŸ”„ Running adaptive clustering...\n",
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 8it [00:00, 25.03it/s, center_shift=0.000000, iteration=8, tol=0.000100]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 10it [00:00, 431.27it/s, center_shift=0.000000, iteration=10, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS - Clusters: 8, High relevance: 6, Passed: True, Tokens: 741, Time: 1.9s\n",
      "ğŸ’¾ Saved: video_0074f737-11cb-497d-8d07-77c3a8127391_groq_pipeline.json\n",
      "\n",
      "ğŸ¯ VIDEO 2/2: 00b9a0de-c59e-49cb-a127-6081e2fb8c8e\n",
      "\n",
      "ğŸ¬ PROCESSING VIDEO: 00b9a0de-c59e-49cb-a127-6081e2fb8c8e\n",
      "============================================================\n",
      "ğŸ“Š Loading features...\n",
      "âœ… Loaded features: shape torch.Size([180, 1024])\n",
      "ğŸ“ Loading captions...\n",
      "âœ… Loaded 180 frame captions\n",
      "â“ Question: ...\n",
      "ğŸ”„ Running adaptive clustering...\n",
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 9it [00:00, 500.52it/s, center_shift=0.000000, iteration=9, tol=0.000100] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 9it [00:00, 438.27it/s, center_shift=0.000000, iteration=9, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 11it [00:00, 311.44it/s, center_shift=0.000000, iteration=11, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 3it [00:00, 187.46it/s, center_shift=0.000000, iteration=3, tol=0.000100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS - Clusters: 32, High relevance: 0, Passed: False, Tokens: 3373, Time: 4.5s\n",
      "ğŸ’¾ Saved: video_00b9a0de-c59e-49cb-a127-6081e2fb8c8e_groq_pipeline.json\n",
      "\n",
      "ğŸ“Š BATCH PROCESSING COMPLETE!\n",
      "================================================================================\n",
      "âœ… Successful: 2\n",
      "âŒ Failed: 0\n",
      "â­ï¸  Skipped: 0\n",
      "ğŸ”¤ Total tokens: 4114\n",
      "ğŸ’¾ Summary saved: ./outputs/batch_processing_summary.json\n",
      "================================================================================\n",
      "âœ… Depth expansion functions loaded!\n",
      "ğŸ“‚ Found 2 Groq result files\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ Depth Expansion Configuration for 00b9a0de-c59e-49cb-a127-6081e2fb8c8e:\n",
      "  - Groq results: ./outputs/video_00b9a0de-c59e-49cb-a127-6081e2fb8c8e_groq_pipeline.json\n",
      "  - Output: depth_expansion_00b9a0de-c59e-49cb-a127-6081e2fb8c8e.json\n",
      "âœ… Groq results loaded successfully!\n",
      "ğŸ“Š Data extracted:\n",
      "  - Total frames: 180\n",
      "  - Clusters: 32\n",
      "  - Representative frames: 32\n",
      "  - Frame relevance: [1, 1, 1, 1, 1]\n",
      "  - Cluster assignments: [10, 31, 31, 31, 31, 31, 31, 23, 19, 31]...\n",
      "âœ… Feature file found: /kaggle/working/extracted_features/00b9a0de-c59e-49cb-a127-6081e2fb8c8e.pt\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ Depth Expansion Configuration for 0074f737-11cb-497d-8d07-77c3a8127391:\n",
      "  - Groq results: ./outputs/video_0074f737-11cb-497d-8d07-77c3a8127391_groq_pipeline.json\n",
      "  - Output: depth_expansion_0074f737-11cb-497d-8d07-77c3a8127391.json\n",
      "âœ… Groq results loaded successfully!\n",
      "ğŸ“Š Data extracted:\n",
      "  - Total frames: 180\n",
      "  - Clusters: 8\n",
      "  - Representative frames: 8\n",
      "  - Frame relevance: [2, 3, 3, 3, 3, 3, 3, 1]\n",
      "  - Cluster assignments: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]...\n",
      "âœ… Feature file found: /kaggle/working/extracted_features/0074f737-11cb-497d-8d07-77c3a8127391.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"GROQ_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = \" \"\n",
    "\n",
    "# Note: Assuming video_ids and questions variables are already defined\n",
    "# video_ids = [list of video IDs]\n",
    "# questions = {video_id: \"Question text with options\", ...}\n",
    "\n",
    "# Cell 1: Imports and Configuration\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "%cd /kaggle/working/kmeans_pytorch\n",
    "from kmeans_pytorch import kmeans\n",
    "import torch\n",
    "import re\n",
    "from groq import Groq\n",
    "\n",
    "\n",
    "%cd /kaggle/working\n",
    "\n",
    "class MultiVideoConfig:\n",
    "    \"\"\"Configuration for multi-video processing with Groq\"\"\"\n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.output_base_path = './outputs'\n",
    "        self.output_filename_template = 'video_{}_groq_pipeline.json'\n",
    "        self.batch_summary_filename = 'batch_processing_summary.json'\n",
    "        \n",
    "        # Video processing settings\n",
    "        self.frame_feat_path = '/kaggle/working/extracted_features'\n",
    "        self.video_ids = video_ids\n",
    "        self.questions = questions  # Use existing questions variable\n",
    "        self.captions_file_path = '/kaggle/input/egoschema/blip2_fullset.json'\n",
    "        \n",
    "        # Clustering parameters\n",
    "        self.max_cluster_num = 32\n",
    "        self.init_cluster_num = 4\n",
    "        self.iter_threshold = 5\n",
    "        self.default_adaptive_rate = 2\n",
    "        \n",
    "        # Groq model configuration\n",
    "        self.model = 'llama-3.1-8b-instant'\n",
    "        self.temperature = 0.0\n",
    "        self.max_tokens = 1000\n",
    "        \n",
    "        # Batch processing settings\n",
    "        self.skip_existing = True\n",
    "        self.delay_between_videos = 1.0\n",
    "        self.max_retries = 3\n",
    "        self.save_intermediate = True\n",
    "\n",
    "# Initialize config\n",
    "config = MultiVideoConfig()\n",
    "print(\"Multi-Video Configuration loaded!\")\n",
    "print(f\"ğŸ“¹ Videos to process: {len(config.video_ids)}\")\n",
    "print(f\"â“ Questions available: {len(config.questions)}\")\n",
    "\n",
    "# Validate that all videos have questions\n",
    "missing_questions = [vid for vid in config.video_ids if vid not in config.questions]\n",
    "if missing_questions:\n",
    "    print(f\"âš ï¸  WARNING: Missing questions for videos: {missing_questions}\")\n",
    "else:\n",
    "    print(\"âœ… All videos have corresponding questions\")\n",
    "\n",
    "\n",
    "# Cell 2: Initialize Groq Model\n",
    "class GroqModel:\n",
    "    \"\"\"Wrapper for Groq API\"\"\"\n",
    "    def __init__(self, model_name='llama-3.1-8b-instant', temperature=0.0, max_tokens=1000):\n",
    "        self.client = Groq()\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    def forward(self, system_prompt, user_prompt):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        try:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                model=self.model_name,\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            \n",
    "            response = chat_completion.choices[0].message.content\n",
    "            \n",
    "            info = {\n",
    "                'response': response,\n",
    "                'model': self.model_name,\n",
    "                'tokens_used': chat_completion.usage.total_tokens if hasattr(chat_completion, 'usage') else 0\n",
    "            }\n",
    "            \n",
    "            return response, info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in Groq API call: {e}\")\n",
    "            return \"\", {'response': '', 'error': str(e)}\n",
    "\n",
    "# Initialize Groq model\n",
    "print(\"Initializing Groq model...\")\n",
    "model = GroqModel(\n",
    "    model_name=config.model,\n",
    "    temperature=config.temperature,\n",
    "    max_tokens=config.max_tokens\n",
    ")\n",
    "print(f\"âœ… Groq model initialized: {config.model}\")\n",
    "\n",
    "\n",
    "# Cell 3: Utility Functions\n",
    "def load_frame_captions(captions_file_path, video_id):\n",
    "    \"\"\"Load frame captions for a specific video from JSON file\"\"\"\n",
    "    try:\n",
    "        captions_data = load_json(captions_file_path)\n",
    "        \n",
    "        if video_id in captions_data:\n",
    "            video_captions_raw = captions_data[video_id]\n",
    "            \n",
    "            # Handle if it's a list instead of dict\n",
    "            if isinstance(video_captions_raw, list):\n",
    "                frame_captions = {}\n",
    "                for i, caption in enumerate(video_captions_raw):\n",
    "                    frame_captions[i] = caption\n",
    "                return frame_captions\n",
    "            else:\n",
    "                # Handle dict format\n",
    "                frame_captions = {}\n",
    "                for key, caption in video_captions_raw.items():\n",
    "                    try:\n",
    "                        frame_idx = int(key)\n",
    "                        frame_captions[frame_idx] = caption\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                return frame_captions\n",
    "        else:\n",
    "            return {}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading captions for {video_id}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_frame_features(video_id, features_folder):\n",
    "    \"\"\"Load frame features for a video\"\"\"\n",
    "    filename = f\"{video_id}.pt\"\n",
    "    filepath = os.path.join(features_folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Feature file not found: {filepath}\")\n",
    "    return torch.load(filepath)\n",
    "\n",
    "def find_closest_points_per_cluster(features, cluster_ids, cluster_centers):\n",
    "    \"\"\"Find closest points to cluster centers\"\"\"\n",
    "    closest_points_idx_per_cluster = {cluster_id: [] for cluster_id in range(len(cluster_centers))}\n",
    "    \n",
    "    for cluster_id in range(len(cluster_centers)):\n",
    "        indices_in_cluster = torch.where(cluster_ids == cluster_id)[0]\n",
    "        points_in_cluster = features[indices_in_cluster]\n",
    "        distances = torch.norm(points_in_cluster - cluster_centers[cluster_id], dim=1)\n",
    "        if distances.numel() > 0:\n",
    "            closest_idx_in_cluster = torch.argmin(distances).item()\n",
    "            closest_global_idx = indices_in_cluster[closest_idx_in_cluster].item()\n",
    "            closest_points_idx_per_cluster[cluster_id].append(closest_global_idx)\n",
    "    \n",
    "    return closest_points_idx_per_cluster\n",
    "\n",
    "def parse_question_text(question_text):\n",
    "    \"\"\"Parse the question text to extract question and options\"\"\"\n",
    "    # lines = question_text.strip().split('\\n')\n",
    "    lines = question_text.replace('\\\\n', '\\n').strip().split('\\n')\n",
    "    \n",
    "    # Find the question line\n",
    "    question = \"\"\n",
    "    options = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Question:\"):\n",
    "            question = line.replace(\"Question:\", \"\").strip()\n",
    "        elif line.startswith(\"Options:\"):\n",
    "            continue\n",
    "        elif line and any(line.startswith(f\"{opt}.\") for opt in [\"A\", \"B\", \"C\", \"D\", \"E\"]):\n",
    "            options.append(line)\n",
    "        elif line.startswith(\"Please choose\"):\n",
    "            break\n",
    "    \n",
    "    return question, options\n",
    "\n",
    "def create_relevance_prompt(tree_node, video_captions, video_id, questions_dict):\n",
    "    \"\"\"Create a focused prompt for relevance scoring using video-specific question\"\"\"\n",
    "    frame_descriptions = []\n",
    "    valid_frames = []\n",
    "    \n",
    "    for frame_idx in tree_node:\n",
    "        if frame_idx in video_captions and video_captions[frame_idx]:\n",
    "            frame_descriptions.append(f\"Frame {frame_idx}: {video_captions[frame_idx]}\")\n",
    "            valid_frames.append(frame_idx)\n",
    "    \n",
    "    if not frame_descriptions:\n",
    "        return None, []\n",
    "\n",
    "    # Get the specific question for this video\n",
    "    if video_id not in questions_dict:\n",
    "        print(f\"âŒ No question found for video {video_id}\")\n",
    "        return None, []\n",
    "    \n",
    "    question_text = questions_dict[video_id]\n",
    "    # question = parse_question_text(question_text)\n",
    "    \n",
    "    if not question_text:\n",
    "        print(f\"âŒ Could not parse question for video {video_id}\")\n",
    "        return None, []\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"VIDEO QUESTION: {question_text}\n",
    "\n",
    "FRAME DESCRIPTIONS:\n",
    "{chr(10).join(frame_descriptions)}\n",
    "\n",
    "TASK: Rate each frame's relevance to answering the question on a scale of 1-3:\n",
    "- 1 = Not relevant (doesn't help answer the question)\n",
    "- 2 = Somewhat relevant (provides some context)  \n",
    "- 3 = Highly relevant (directly helps answer the question)\n",
    "\n",
    "Provide ONLY the relevance scores in this exact format:\n",
    "frame relevance: [score1, score2, score3, ...]\n",
    "\n",
    "You must provide exactly {len(frame_descriptions)} scores for the {len(frame_descriptions)} frames described above.\n",
    "\n",
    "Example: frame relevance: [2, 1, 3, 2, 1]\"\"\"\n",
    "\n",
    "    return prompt, valid_frames\n",
    "\n",
    "def extract_relevance_scores(text):\n",
    "    \"\"\"Extract relevance scores from model response\"\"\"\n",
    "    response = text.strip()\n",
    "    \n",
    "    patterns = [\n",
    "        r\"frame relevance:\\s*\\[([0-9,\\s]+)\\]\",\n",
    "        r\"relevance:\\s*\\[([0-9,\\s]+)\\]\", \n",
    "        r\"scores:\\s*\\[([0-9,\\s]+)\\]\",\n",
    "        r\"relevance scores:\\s*\\[([0-9,\\s]+)\\]\",\n",
    "        r\"\\[([0-9,\\s]+)\\]\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        relevance_match = re.search(pattern, response, re.IGNORECASE)\n",
    "        if relevance_match:\n",
    "            try:\n",
    "                numbers_str = relevance_match.group(1)\n",
    "                relevance = [int(x.strip()) for x in numbers_str.split(',') if x.strip().isdigit()]\n",
    "                relevance = [max(1, min(3, score)) for score in relevance]\n",
    "                return relevance\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Batch processing utilities\n",
    "def check_existing_result(video_id, config):\n",
    "    \"\"\"Check if result already exists for this video\"\"\"\n",
    "    output_filename = config.output_filename_template.format(video_id)\n",
    "    output_path = os.path.join(config.output_base_path, output_filename)\n",
    "    return os.path.exists(output_path)\n",
    "\n",
    "def save_batch_summary(batch_results, config):\n",
    "    \"\"\"Save summary of batch processing\"\"\"\n",
    "    summary = {\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'total_videos': len(config.video_ids),\n",
    "        'processed_videos': len([r for r in batch_results if r['status'] == 'success']),\n",
    "        'failed_videos': len([r for r in batch_results if r['status'] == 'error']),\n",
    "        'skipped_videos': len([r for r in batch_results if r['status'] == 'skipped']),\n",
    "        'total_tokens_used': sum(r.get('tokens_used', 0) for r in batch_results),\n",
    "        'results': batch_results,\n",
    "        'config': {k: v for k, v in vars(config).items() if not k.startswith('_') and k not in ['video_ids', 'questions']}\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(config.output_base_path, config.batch_summary_filename)\n",
    "    save_json(summary, summary_path)\n",
    "    return summary_path\n",
    "\n",
    "# Ensure output directory exists\n",
    "makedir(config.output_base_path)\n",
    "print(\"âœ… Utility functions loaded!\")\n",
    "\n",
    "\n",
    "# Cell 4: Adaptive Clustering Function\n",
    "def adaptive_clustering_with_groq(frame_feats, config, model, video_id, video_captions):\n",
    "    \"\"\"Perform adaptive clustering with Groq model for relevance prediction\"\"\"\n",
    "    cluster_num = config.init_cluster_num\n",
    "    device = frame_feats.device\n",
    "    \n",
    "    clustering_results = []\n",
    "    \n",
    "    while cluster_num <= config.max_cluster_num:\n",
    "        try:\n",
    "            # Perform k-means clustering\n",
    "            cluster_ids_x, cluster_centers = kmeans(\n",
    "                X=frame_feats, \n",
    "                num_clusters=cluster_num, \n",
    "                distance='cosine', \n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            cluster_ids_x = cluster_ids_x.to(device)\n",
    "            cluster_centers = cluster_centers.to(device)\n",
    "            \n",
    "            # Find representative frames\n",
    "            closest_points_idx_per_cluster = find_closest_points_per_cluster(\n",
    "                frame_feats, cluster_ids_x, cluster_centers\n",
    "            )\n",
    "            \n",
    "            if not closest_points_idx_per_cluster:\n",
    "                cluster_num *= config.default_adaptive_rate\n",
    "                continue\n",
    "            \n",
    "            # Get representative frame indices\n",
    "            tree_node = sorted([value for sublist in closest_points_idx_per_cluster.values() for value in sublist])\n",
    "            cluster_ids_list = cluster_ids_x.tolist()\n",
    "            \n",
    "            # Create relevance prompt using video-specific question\n",
    "            prompt, valid_frames = create_relevance_prompt(tree_node, video_captions, video_id, config.questions)\n",
    "            \n",
    "            if not prompt:\n",
    "                cluster_num *= config.default_adaptive_rate\n",
    "                continue\n",
    "            \n",
    "            # Model inference with Groq\n",
    "            response, info = model.forward(\n",
    "                \"You are an expert video analyst. Analyze frame descriptions and rate their relevance to answering the specific question.\", \n",
    "                prompt\n",
    "            )\n",
    "            \n",
    "            # Extract frame relevance\n",
    "            frame_relevance = extract_relevance_scores(response)\n",
    "            \n",
    "            # Count high relevance frames (score = 3)\n",
    "            if isinstance(frame_relevance, list) and len(frame_relevance) == len(valid_frames):\n",
    "                high_relevance_frame_num = frame_relevance.count(3)\n",
    "            else:\n",
    "                high_relevance_frame_num = 0\n",
    "            \n",
    "            # Store clustering result\n",
    "            clustering_result = {\n",
    "                'num_clusters': cluster_num,\n",
    "                'actual_clusters': len(set(cluster_ids_list)),\n",
    "                'representative_frames': tree_node,\n",
    "                'valid_frames_with_captions': valid_frames,\n",
    "                'cluster_assignments': cluster_ids_list,\n",
    "                'frame_relevance': frame_relevance,\n",
    "                'high_relevance_count': high_relevance_frame_num,\n",
    "                'prompt': prompt,\n",
    "                'model_response': info.get('response', ''),\n",
    "                'tokens_used': info.get('tokens_used', 0)\n",
    "            }\n",
    "            clustering_results.append(clustering_result)\n",
    "            \n",
    "            # Check stopping condition\n",
    "            if high_relevance_frame_num < config.iter_threshold:\n",
    "                if cluster_num < config.max_cluster_num:\n",
    "                    next_cluster_num = cluster_num * config.default_adaptive_rate\n",
    "                    cluster_num = next_cluster_num\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Clustering failed with {cluster_num} clusters: {e}\")\n",
    "            cluster_num *= config.default_adaptive_rate\n",
    "            continue\n",
    "    \n",
    "    # Return the final successful clustering result\n",
    "    if clustering_results:\n",
    "        final_result = clustering_results[-1]\n",
    "        return (final_result['representative_frames'], \n",
    "                final_result['cluster_assignments'], \n",
    "                final_result['frame_relevance'],\n",
    "                final_result['high_relevance_count'],\n",
    "                clustering_results)\n",
    "    else:\n",
    "        return [], [], [], 0, []\n",
    "\n",
    "\n",
    "# Cell 5: Single Video Processing Function\n",
    "def process_single_video(video_id, config, model, device):\n",
    "    \"\"\"Process a single video and return results\"\"\"\n",
    "    print(f\"\\nğŸ¬ PROCESSING VIDEO: {video_id}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Check if feature file exists\n",
    "        feature_file_path = os.path.join(config.frame_feat_path, f\"{video_id}.pt\")\n",
    "        if not os.path.exists(feature_file_path):\n",
    "            return {\n",
    "                'video_id': video_id,\n",
    "                'status': 'error',\n",
    "                'error': f'Feature file not found: {feature_file_path}',\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "        \n",
    "        # Check if question exists for this video\n",
    "        if video_id not in config.questions:\n",
    "            return {\n",
    "                'video_id': video_id,\n",
    "                'status': 'error',\n",
    "                'error': f'No question found for video: {video_id}',\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "        \n",
    "        # Load frame features\n",
    "        print(f\"ğŸ“Š Loading features...\")\n",
    "        frame_feats = load_frame_features(video_id, config.frame_feat_path)\n",
    "        frame_feats = frame_feats.to(device)\n",
    "        print(f\"âœ… Loaded features: shape {frame_feats.shape}\")\n",
    "        \n",
    "        # Load frame captions\n",
    "        print(f\"ğŸ“ Loading captions...\")\n",
    "        video_captions = load_frame_captions(config.captions_file_path, video_id)\n",
    "        if not video_captions:\n",
    "            return {\n",
    "                'video_id': video_id,\n",
    "                'status': 'error',\n",
    "                'error': 'No captions found for video',\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "        print(f\"âœ… Loaded {len(video_captions)} frame captions\")\n",
    "        \n",
    "        # Show the question for this video\n",
    "        question_text = config.questions[video_id]\n",
    "        question, options = parse_question_text(question_text)\n",
    "        print(f\"â“ Question: {question[:100]}...\")\n",
    "        \n",
    "        # Run adaptive clustering\n",
    "        print(f\"ğŸ”„ Running adaptive clustering...\")\n",
    "        (representative_frames, cluster_assignments, \n",
    "         frame_relevance, high_relevance_count, all_results) = adaptive_clustering_with_groq(\n",
    "            frame_feats, config, model, video_id, video_captions\n",
    "        )\n",
    "        \n",
    "        # Create result\n",
    "        result = {\n",
    "            'video_id': video_id,\n",
    "            'status': 'success',\n",
    "            'processing_time': time.time() - start_time,\n",
    "            'feature_file_path': feature_file_path,\n",
    "            'total_frames': len(frame_feats),\n",
    "            'feature_dimensions': frame_feats.shape[1] if len(frame_feats.shape) > 1 else 1,\n",
    "            'question_text': question_text,\n",
    "            'parsed_question': question,\n",
    "            'parsed_options': options,\n",
    "            'final_result': {\n",
    "                'representative_frames': representative_frames,\n",
    "                'cluster_assignments': cluster_assignments,\n",
    "                'frame_relevance': frame_relevance,\n",
    "                'high_relevance_count': high_relevance_count,\n",
    "                'num_clusters': len(set(cluster_assignments)) if cluster_assignments else 0,\n",
    "                'passed_threshold': high_relevance_count >= config.iter_threshold\n",
    "            },\n",
    "            'all_clustering_attempts': all_results,\n",
    "            'tokens_used': sum(attempt.get('tokens_used', 0) for attempt in all_results)\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        final = result['final_result']\n",
    "        print(f\"âœ… SUCCESS - Clusters: {final['num_clusters']}, \"\n",
    "              f\"High relevance: {final['high_relevance_count']}, \"\n",
    "              f\"Passed: {final['passed_threshold']}, \"\n",
    "              f\"Tokens: {result['tokens_used']}, \"\n",
    "              f\"Time: {result['processing_time']:.1f}s\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR processing {video_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'video_id': video_id,\n",
    "            'status': 'error',\n",
    "            'error': str(e),\n",
    "            'processing_time': time.time() - start_time\n",
    "        }\n",
    "\n",
    "\n",
    "# Cell 6: Main Batch Processing Loop\n",
    "def run_batch_processing():\n",
    "    \"\"\"Run the complete batch processing pipeline\"\"\"\n",
    "    print(\"ğŸš€ STARTING BATCH PROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“¹ Total videos to process: {len(config.video_ids)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_results = []\n",
    "    \n",
    "    # Process each video\n",
    "    for i, video_id in enumerate(config.video_ids, 1):\n",
    "        print(f\"\\nğŸ¯ VIDEO {i}/{len(config.video_ids)}: {video_id}\")\n",
    "        \n",
    "        # Check if we should skip existing results\n",
    "        if config.skip_existing and check_existing_result(video_id, config):\n",
    "            print(f\"â­ï¸  SKIPPED - Result already exists\")\n",
    "            batch_results.append({\n",
    "                'video_id': video_id,\n",
    "                'status': 'skipped',\n",
    "                'reason': 'Result already exists'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Process video with retries\n",
    "        retry_count = 0\n",
    "        result = None\n",
    "        \n",
    "        while retry_count < config.max_retries:\n",
    "            try:\n",
    "                result = process_single_video(video_id, config, model, device)\n",
    "                if result['status'] == 'success':\n",
    "                    break\n",
    "                else:\n",
    "                    retry_count += 1\n",
    "                    if retry_count < config.max_retries:\n",
    "                        print(f\"ğŸ”„ Retrying ({retry_count + 1}/{config.max_retries})...\")\n",
    "                        time.sleep(config.delay_between_videos)\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                if retry_count >= config.max_retries:\n",
    "                    result = {\n",
    "                        'video_id': video_id,\n",
    "                        'status': 'error',\n",
    "                        'error': f'Failed after {config.max_retries} retries: {str(e)}'\n",
    "                    }\n",
    "        \n",
    "        batch_results.append(result)\n",
    "        \n",
    "        # Save individual result\n",
    "        if config.save_intermediate and result['status'] == 'success':\n",
    "            output_filename = config.output_filename_template.format(video_id)\n",
    "            output_path = os.path.join(config.output_base_path, output_filename)\n",
    "            save_json(result, output_path)\n",
    "            print(f\"ğŸ’¾ Saved: {output_filename}\")\n",
    "        \n",
    "        # Rate limiting delay\n",
    "        if i < len(config.video_ids):  # Don't delay after last video\n",
    "            time.sleep(config.delay_between_videos)\n",
    "    \n",
    "    # Save batch summary\n",
    "    print(f\"\\nğŸ“Š BATCH PROCESSING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_path = save_batch_summary(batch_results, config)\n",
    "    \n",
    "    # Print final statistics\n",
    "    successful = len([r for r in batch_results if r['status'] == 'success'])\n",
    "    failed = len([r for r in batch_results if r['status'] == 'error'])\n",
    "    skipped = len([r for r in batch_results if r['status'] == 'skipped'])\n",
    "    total_tokens = sum(r.get('tokens_used', 0) for r in batch_results)\n",
    "    \n",
    "    print(f\"âœ… Successful: {successful}\")\n",
    "    print(f\"âŒ Failed: {failed}\")\n",
    "    print(f\"â­ï¸  Skipped: {skipped}\")\n",
    "    print(f\"ğŸ”¤ Total tokens: {total_tokens}\")\n",
    "    print(f\"ğŸ’¾ Summary saved: {summary_path}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "# Run the batch processing\n",
    "batch_results = run_batch_processing()\n",
    "\n",
    "\n",
    "# Cell 1: Depth Expansion - Imports and Functions\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def hierarchical_clustering_with_external_primary(video_features, cluster_ids, relevance_scores, num_subclusters=5, num_subsubclusters=5):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering based on relevance scores:\n",
    "    - Score 1: Keep only primary cluster\n",
    "    - Score 2: Split into subclusters  \n",
    "    - Score 3: Split into sub-subclusters\n",
    "    \"\"\"\n",
    "    clusters = {i: {} for i in range(0, max(cluster_ids)+1)}\n",
    "\n",
    "    for cluster_id in set(cluster_ids):\n",
    "        primary_indices = [i for i, x in enumerate(cluster_ids) if x == cluster_id]\n",
    "\n",
    "        if cluster_id < len(relevance_scores):\n",
    "            score = relevance_scores[cluster_id]\n",
    "        else:\n",
    "            score = 3\n",
    "\n",
    "        if len(primary_indices) < 2:\n",
    "            clusters[cluster_id] = primary_indices\n",
    "            continue\n",
    "\n",
    "        sub_features = video_features[primary_indices]\n",
    "\n",
    "        if score == 1:\n",
    "            # Low relevance: keep as single cluster\n",
    "            clusters[cluster_id] = primary_indices\n",
    "            continue\n",
    "\n",
    "        # Create subclusters\n",
    "        linked_sub = linkage(sub_features, method='ward')\n",
    "        sub_cluster_labels = fcluster(linked_sub, num_subclusters, criterion='maxclust')\n",
    "        sub_cluster_labels = sub_cluster_labels - 1\n",
    "\n",
    "        if score == 2:\n",
    "            # Medium relevance: split into subclusters\n",
    "            clusters[cluster_id] = {i: [primary_indices[j] for j in np.where(sub_cluster_labels == i)[0]] for i in range(0, num_subclusters)}\n",
    "            continue\n",
    "\n",
    "        # High relevance (score == 3): split into sub-subclusters\n",
    "        clusters[cluster_id] = {}\n",
    "        for subcluster_id in range(0, num_subclusters):\n",
    "            sub_indices = np.where(sub_cluster_labels == subcluster_id)[0]\n",
    "            if len(sub_indices) < 2:\n",
    "                continue\n",
    "\n",
    "            subsub_features = sub_features[sub_indices]\n",
    "            linked_subsub = linkage(subsub_features, method='ward')\n",
    "            subsub_cluster_labels = fcluster(linked_subsub, num_subsubclusters, criterion='maxclust')\n",
    "            subsub_cluster_labels = subsub_cluster_labels - 1\n",
    "\n",
    "            clusters[cluster_id][subcluster_id] = {}\n",
    "            for subsubcluster_id in range(0, num_subsubclusters):\n",
    "                final_indices = sub_indices[np.where(subsub_cluster_labels == subsubcluster_id)[0]]\n",
    "                original_indices = [primary_indices[i] for i in final_indices]\n",
    "                clusters[cluster_id][subcluster_id][subsubcluster_id] = original_indices\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def cosine_similarity(points, centroid):\n",
    "    \"\"\"Calculate cosine similarity between points and centroid.\"\"\"\n",
    "    points_normalized = F.normalize(points, dim=1)\n",
    "    centroid_normalized = F.normalize(centroid.unsqueeze(0), dim=1)\n",
    "    return 1 - torch.mm(points_normalized, centroid_normalized.T).squeeze()\n",
    "\n",
    "def find_closest_points_in_temporal_order_subsub(x, clusters, relevance_scores):\n",
    "    \"\"\"Find representative frames from hierarchical clusters in temporal order.\"\"\"\n",
    "    closest_points_indices = []\n",
    "\n",
    "    for cluster_id, cluster_data in clusters.items():\n",
    "        if cluster_id < len(relevance_scores):\n",
    "            relevance = relevance_scores[cluster_id]\n",
    "        else:\n",
    "            relevance = 3\n",
    "\n",
    "        if isinstance(cluster_data, list):  # Primary cluster directly\n",
    "            cluster_data = np.array(cluster_data)\n",
    "            if cluster_data.size == 0:\n",
    "                continue\n",
    "            points_in_cluster = x[torch.tensor(cluster_data, dtype=torch.long)]\n",
    "            cluster_centroid = points_in_cluster.mean(dim=0)\n",
    "            distances = cosine_similarity(points_in_cluster, cluster_centroid)\n",
    "            if distances.numel() > 0:\n",
    "                closest_idx = torch.argmin(distances).item()\n",
    "                closest_points_indices.append(int(cluster_data[closest_idx]))\n",
    "\n",
    "        elif isinstance(cluster_data, dict):  # Handle subclusters and sub-subclusters\n",
    "            if relevance == 1:\n",
    "                # Only take representative frame for primary cluster\n",
    "                primary_indices = []\n",
    "                for subcluster_data in cluster_data.values():\n",
    "                    if isinstance(subcluster_data, dict):\n",
    "                        for sub_data in subcluster_data.values():\n",
    "                            if len(sub_data) > 0:\n",
    "                                primary_indices.extend(sub_data)\n",
    "                    elif isinstance(subcluster_data, list) and len(subcluster_data) > 0:\n",
    "                        primary_indices.extend(subcluster_data)\n",
    "\n",
    "                if primary_indices:\n",
    "                    primary_indices = np.array(primary_indices)\n",
    "                    primary_points = x[torch.tensor(primary_indices, dtype=torch.long)]\n",
    "                    primary_centroid = primary_points.mean(dim=0)\n",
    "                    primary_distances = cosine_similarity(primary_points, primary_centroid)\n",
    "                    if primary_distances.numel() > 0:\n",
    "                        closest_primary_idx = torch.argmin(primary_distances).item()\n",
    "                        closest_points_indices.append(int(primary_indices[closest_primary_idx]))\n",
    "                continue\n",
    "\n",
    "            elif relevance == 2 or relevance == 3:\n",
    "                # Include primary cluster representative\n",
    "                primary_indices = []\n",
    "                for subcluster_data in cluster_data.values():\n",
    "                    if isinstance(subcluster_data, dict):\n",
    "                        for sub_data in subcluster_data.values():\n",
    "                            if len(sub_data) > 0:\n",
    "                                primary_indices.extend(sub_data)\n",
    "                    elif isinstance(subcluster_data, list) and len(subcluster_data) > 0:\n",
    "                        primary_indices.extend(subcluster_data)\n",
    "\n",
    "                if primary_indices:\n",
    "                    primary_indices = np.array(primary_indices)\n",
    "                    primary_points = x[torch.tensor(primary_indices, dtype=torch.long)]\n",
    "                    primary_centroid = primary_points.mean(dim=0)\n",
    "                    primary_distances = cosine_similarity(primary_points, primary_centroid)\n",
    "                    if primary_distances.numel() > 0:\n",
    "                        closest_primary_idx = torch.argmin(primary_distances).item()\n",
    "                        closest_points_indices.append(int(primary_indices[closest_primary_idx]))\n",
    "\n",
    "                # Process subclusters/sub-subclusters\n",
    "                for subcluster_id, subclusters in cluster_data.items():\n",
    "                    if isinstance(subclusters, dict):  # Sub-subclusters\n",
    "                        for subsubcluster_id, indices in subclusters.items():\n",
    "                            if len(indices) == 0:\n",
    "                                continue\n",
    "                            indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "                            points_in_subsubcluster = x[indices_tensor]\n",
    "                            subsubcluster_centroid = points_in_subsubcluster.mean(dim=0)\n",
    "                            distances = cosine_similarity(points_in_subsubcluster, subsubcluster_centroid)\n",
    "                            if distances.numel() > 0:\n",
    "                                closest_idx_in_subsubcluster = torch.argmin(distances).item()\n",
    "                                closest_global_idx = indices[closest_idx_in_subsubcluster]\n",
    "                                closest_points_indices.append(int(closest_global_idx))\n",
    "\n",
    "                    elif isinstance(subclusters, list):\n",
    "                        subclusters = np.array(subclusters)\n",
    "                        if subclusters.size == 0:\n",
    "                            continue\n",
    "                        points_in_subcluster = x[torch.tensor(subclusters, dtype=torch.long)]\n",
    "                        subcluster_centroid = points_in_subcluster.mean(dim=0)\n",
    "                        distances = cosine_similarity(points_in_subcluster, subcluster_centroid)\n",
    "                        if distances.numel() > 0:\n",
    "                            closest_idx = torch.argmin(distances).item()\n",
    "                            closest_points_indices.append(int(subclusters[closest_idx]))\n",
    "\n",
    "    closest_points_indices.sort()  # Ensure temporal order\n",
    "    return closest_points_indices\n",
    "\n",
    "def load_image_features(name_ids, save_folder):\n",
    "    \"\"\"Load image features from a .pt file.\"\"\"\n",
    "    filename = f\"{name_ids}.pt\"\n",
    "    filepath = os.path.join(save_folder, filename)\n",
    "    img_feats = torch.load(filepath)\n",
    "    return img_feats\n",
    "\n",
    "def load_json(fn):\n",
    "    \"\"\"Load JSON file.\"\"\"\n",
    "    with open(fn, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def save_json(data, fn, indent=4):\n",
    "    \"\"\"Save data to JSON file.\"\"\"\n",
    "    with open(fn, 'w') as f:\n",
    "        json.dump(data, f, indent=indent)\n",
    "\n",
    "print(\"âœ… Depth expansion functions loaded!\")\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# Configuration for depth expansion\n",
    "class DepthExpansionConfig:\n",
    "    def __init__(self, video_id, json_path):\n",
    "        # Input paths\n",
    "        self.save_folder = '/kaggle/working/extracted_features'  # Where your .pt files are\n",
    "        self.video_id = video_id  # Extracted from filename\n",
    "        \n",
    "        # Previous results from Groq pipeline\n",
    "        self.groq_results_path = json_path\n",
    "        \n",
    "        # Output paths\n",
    "        self.output_base_path = './outputs'\n",
    "        self.output_filename = f'depth_expansion_{video_id}.json'\n",
    "        \n",
    "        # Hierarchical clustering parameters\n",
    "        self.num_subclusters = 4\n",
    "        self.num_subsubclusters = 4\n",
    "\n",
    "# Find all Groq pipeline results\n",
    "json_files = glob(\"./outputs/*_groq_pipeline.json\")\n",
    "\n",
    "if not json_files:\n",
    "    print(\"âŒ No Groq pipeline result JSON files found in ./outputs/\")\n",
    "else:\n",
    "    print(f\"ğŸ“‚ Found {len(json_files)} Groq result files\")\n",
    "\n",
    "# Loop through each file\n",
    "for json_path in json_files:\n",
    "    # Extract video_id from filename\n",
    "    base_name = os.path.basename(json_path)\n",
    "    video_id = base_name.replace(\"video_\", \"\").replace(\"_groq_pipeline.json\", \"\")\n",
    "    \n",
    "    config = DepthExpansionConfig(video_id, json_path)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ“‹ Depth Expansion Configuration for {video_id}:\")\n",
    "    print(f\"  - Groq results: {config.groq_results_path}\")\n",
    "    print(f\"  - Output: {config.output_filename}\")\n",
    "    \n",
    "    # Load previous Groq pipeline results\n",
    "    try:\n",
    "        with open(config.groq_results_path, 'r') as f:\n",
    "            groq_results = json.load(f)\n",
    "        \n",
    "        print(\"âœ… Groq results loaded successfully!\")\n",
    "        \n",
    "        # Extract needed data\n",
    "        cluster_assignments = groq_results['final_result']['cluster_assignments']\n",
    "        frame_relevance = groq_results['final_result']['frame_relevance'] \n",
    "        representative_frames = groq_results['final_result']['representative_frames']\n",
    "        \n",
    "        print(f\"ğŸ“Š Data extracted:\")\n",
    "        print(f\"  - Total frames: {groq_results['total_frames']}\")\n",
    "        print(f\"  - Clusters: {len(set(cluster_assignments))}\")\n",
    "        print(f\"  - Representative frames: {len(representative_frames)}\")\n",
    "        print(f\"  - Frame relevance: {frame_relevance}\")\n",
    "        print(f\"  - Cluster assignments: {cluster_assignments[:10]}...\")  # Show first 10\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Error: Could not find Groq results at {config.groq_results_path}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading Groq results: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Check if feature file exists\n",
    "    feature_path = os.path.join(config.save_folder, f\"{config.video_id}.pt\")\n",
    "    if os.path.exists(feature_path):\n",
    "        print(f\"âœ… Feature file found: {feature_path}\")\n",
    "    else:\n",
    "        print(f\"âŒ Feature file not found: {feature_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:08:20.103145Z",
     "iopub.status.busy": "2025-08-09T18:08:20.102622Z",
     "iopub.status.idle": "2025-08-09T18:08:20.190896Z",
     "shell.execute_reply": "2025-08-09T18:08:20.190142Z",
     "shell.execute_reply.started": "2025-08-09T18:08:20.103124Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ STARTING DEPTH EXPANSION for video: 0074f737-11cb-497d-8d07-77c3a8127391\n",
      "============================================================\n",
      "âœ… Loaded Groq results for video 0074f737-11cb-497d-8d07-77c3a8127391\n",
      "ğŸ“‚ Loading features for 0074f737-11cb-497d-8d07-77c3a8127391...\n",
      "âœ… Features loaded: shape torch.Size([180, 1024])\n",
      "ğŸ“Š Processing relevance scores...\n",
      "âœ… Using extracted relevance scores: [2, 3, 3, 3, 3, 3, 3, 1]\n",
      "ğŸ“ˆ Relevance scores: [2, 3, 3, 3, 3, 3, 3, 1]\n",
      "ğŸ¯ Cluster assignments: 180 total assignments\n",
      "\n",
      "ğŸ”„ Performing hierarchical clustering...\n",
      "  - Primary clusters: 8\n",
      "  - Subclusters per cluster: 4\n",
      "  - Sub-subclusters per subcluster: 4\n",
      "âœ… Hierarchical clustering complete!\n",
      "ğŸ“Š Clusters info type: <class 'dict'>\n",
      "\n",
      "ğŸ¯ Finding representative points in temporal order...\n",
      "âœ… Representative points found!\n",
      "ğŸ“ Number of representative points: 86\n",
      "ğŸ• Temporal order: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 20, 20, 22, 30, 31, 34, 40, 41, 53, 56, 62, 63, 64, 65, 67, 68, 70, 72, 74, 75, 79, 80, 83, 85, 87, 87, 88, 89, 90, 91, 92, 93, 102, 105, 107, 111, 114, 115, 116, 117, 118, 119, 121, 122, 124, 126, 126, 127, 128, 131, 140, 141, 141, 149, 151, 152, 153, 154, 155, 155, 156, 158, 160, 161, 162, 165, 168, 169, 169, 174, 175, 177]\n",
      "\n",
      "ğŸ“Š COMPARISON:\n",
      "  Original (width expansion): [7, 20, 54, 87, 126, 141, 152, 169]\n",
      "  New (depth expansion): [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 20, 20, 22, 30, 31, 34, 40, 41, 53, 56, 62, 63, 64, 65, 67, 68, 70, 72, 74, 75, 79, 80, 83, 85, 87, 87, 88, 89, 90, 91, 92, 93, 102, 105, 107, 111, 114, 115, 116, 117, 118, 119, 121, 122, 124, 126, 126, 127, 128, 131, 140, 141, 141, 149, 151, 152, 153, 154, 155, 155, 156, 158, 160, 161, 162, 165, 168, 169, 169, 174, 175, 177]\n",
      "  Original count: 8\n",
      "  New count: 86\n",
      "  ğŸ“ˆ Expansion ratio: 10.75x\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "ğŸš€ STARTING DEPTH EXPANSION for video: 00b9a0de-c59e-49cb-a127-6081e2fb8c8e\n",
      "============================================================\n",
      "âœ… Loaded Groq results for video 00b9a0de-c59e-49cb-a127-6081e2fb8c8e\n",
      "ğŸ“‚ Loading features for 00b9a0de-c59e-49cb-a127-6081e2fb8c8e...\n",
      "âœ… Features loaded: shape torch.Size([180, 1024])\n",
      "ğŸ“Š Processing relevance scores...\n",
      "âœ… Using extracted relevance scores: [1, 1, 1, 1, 1]\n",
      "ğŸ“ˆ Relevance scores: [1, 1, 1, 1, 1]\n",
      "ğŸ¯ Cluster assignments: 180 total assignments\n",
      "\n",
      "ğŸ”„ Performing hierarchical clustering...\n",
      "  - Primary clusters: 32\n",
      "  - Subclusters per cluster: 4\n",
      "  - Sub-subclusters per subcluster: 4\n",
      "âœ… Hierarchical clustering complete!\n",
      "ğŸ“Š Clusters info type: <class 'dict'>\n",
      "\n",
      "ğŸ¯ Finding representative points in temporal order...\n",
      "âœ… Representative points found!\n",
      "ğŸ“ Number of representative points: 98\n",
      "ğŸ• Temporal order: [1, 2, 3, 3, 4, 5, 6, 10, 11, 12, 13, 14, 17, 17, 18, 19, 21, 23, 24, 25, 26, 30, 31, 34, 46, 48, 49, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 66, 68, 71, 72, 73, 87, 88, 91, 96, 97, 98, 103, 105, 106, 107, 108, 110, 114, 114, 115, 121, 122, 122, 123, 124, 130, 130, 133, 136, 139, 141, 142, 144, 146, 147, 148, 151, 152, 153, 154, 154, 155, 156, 156, 158, 159, 159, 160, 161, 162, 163, 163, 164, 167, 170, 171, 171, 174, 176, 177, 179]\n",
      "\n",
      "ğŸ“Š COMPARISON:\n",
      "  Original (width expansion): [5, 7, 15, 17, 20, 21, 27, 31, 41, 47, 56, 59, 62, 74, 77, 98, 100, 103, 110, 114, 117, 118, 122, 130, 141, 153, 156, 159, 163, 167, 171, 179]\n",
      "  New (depth expansion): [1, 2, 3, 3, 4, 5, 6, 10, 11, 12, 13, 14, 17, 17, 18, 19, 21, 23, 24, 25, 26, 30, 31, 34, 46, 48, 49, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 66, 68, 71, 72, 73, 87, 88, 91, 96, 97, 98, 103, 105, 106, 107, 108, 110, 114, 114, 115, 121, 122, 122, 123, 124, 130, 130, 133, 136, 139, 141, 142, 144, 146, 147, 148, 151, 152, 153, 154, 154, 155, 156, 156, 158, 159, 159, 160, 161, 162, 163, 163, 164, 167, 170, 171, 171, 174, 176, 177, 179]\n",
      "  Original count: 32\n",
      "  New count: 98\n",
      "  ğŸ“ˆ Expansion ratio: 3.06x\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "for vid in video_ids:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸš€ STARTING DEPTH EXPANSION for video: {vid}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Setup config for current video\n",
    "    groq_path = f'./outputs/video_{vid}_groq_pipeline.json'\n",
    "    \n",
    "    # config = DepthExpansionConfig()\n",
    "    config = DepthExpansionConfig(vid, groq_path)  # Pass args here\n",
    "    config.video_id = vid\n",
    "    config.groq_results_path = f'./outputs/video_{vid}_groq_pipeline.json'\n",
    "    config.output_filename = f'depth_expansion_{vid}.json'\n",
    "\n",
    "    # Load Groq results JSON for this video\n",
    "    try:\n",
    "        with open(config.groq_results_path, 'r') as f:\n",
    "            groq_results = json.load(f)\n",
    "        \n",
    "        cluster_assignments = groq_results['final_result']['cluster_assignments']\n",
    "        frame_relevance = groq_results['final_result']['frame_relevance'] \n",
    "        representative_frames = groq_results['final_result']['representative_frames']\n",
    "\n",
    "        print(f\"âœ… Loaded Groq results for video {vid}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Groq results not found for video {vid}, skipping...\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading Groq results for video {vid}: {e}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Load features for this video\n",
    "    print(f\"ğŸ“‚ Loading features for {vid}...\")\n",
    "    try:\n",
    "        img_feats = load_image_features(vid, config.save_folder)\n",
    "        img_feats = img_feats.cpu()  # For scipy\n",
    "        print(f\"âœ… Features loaded: shape {img_feats.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load features for {vid}: {e}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Process relevance scores\n",
    "    print(f\"ğŸ“Š Processing relevance scores...\")\n",
    "    if isinstance(frame_relevance, list) and len(frame_relevance) > 0:\n",
    "        print(f\"âœ… Using extracted relevance scores: {frame_relevance}\")\n",
    "        relevance_scores = frame_relevance\n",
    "    else:\n",
    "        print(\"âš ï¸  No relevance scores found, using default scores...\")\n",
    "        num_clusters = len(set(cluster_assignments))\n",
    "        relevance_scores = [2] * num_clusters  # Medium relevance\n",
    "\n",
    "    print(f\"ğŸ“ˆ Relevance scores: {relevance_scores}\")\n",
    "    print(f\"ğŸ¯ Cluster assignments: {len(cluster_assignments)} total assignments\")\n",
    "\n",
    "    # Perform hierarchical clustering\n",
    "    print(f\"\\nğŸ”„ Performing hierarchical clustering...\")\n",
    "    print(f\"  - Primary clusters: {len(set(cluster_assignments))}\")\n",
    "    print(f\"  - Subclusters per cluster: {config.num_subclusters}\")\n",
    "    print(f\"  - Sub-subclusters per subcluster: {config.num_subsubclusters}\")\n",
    "\n",
    "    clusters_info = hierarchical_clustering_with_external_primary(\n",
    "        img_feats,\n",
    "        cluster_assignments,\n",
    "        relevance_scores,\n",
    "        num_subclusters=config.num_subclusters,\n",
    "        num_subsubclusters=config.num_subsubclusters\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Hierarchical clustering complete!\")\n",
    "    print(f\"ğŸ“Š Clusters info type: {type(clusters_info)}\")\n",
    "\n",
    "    # Find representative points in temporal order\n",
    "    print(f\"\\nğŸ¯ Finding representative points in temporal order...\")\n",
    "    closest_points_temporal = find_closest_points_in_temporal_order_subsub(\n",
    "        img_feats,\n",
    "        clusters_info,\n",
    "        relevance_scores\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… Representative points found!\")\n",
    "    print(f\"ğŸ“ Number of representative points: {len(closest_points_temporal)}\")\n",
    "    print(f\"ğŸ• Temporal order: {closest_points_temporal}\")\n",
    "\n",
    "    # Compare with original representative frames\n",
    "    print(f\"\\nğŸ“Š COMPARISON:\")\n",
    "    print(f\"  Original (width expansion): {representative_frames}\")\n",
    "    print(f\"  New (depth expansion): {closest_points_temporal}\")\n",
    "    print(f\"  Original count: {len(representative_frames)}\")\n",
    "    print(f\"  New count: {len(closest_points_temporal)}\")\n",
    "\n",
    "    # Calculate expansion ratio\n",
    "    if len(representative_frames) > 0:\n",
    "        expansion_ratio = len(closest_points_temporal) / len(representative_frames)\n",
    "        print(f\"  ğŸ“ˆ Expansion ratio: {expansion_ratio:.2f}x\")\n",
    "    else:\n",
    "        expansion_ratio = 0\n",
    "        print(f\"  âŒ No original frames to compare\")\n",
    "\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:08:27.926849Z",
     "iopub.status.busy": "2025-08-09T18:08:27.926106Z",
     "iopub.status.idle": "2025-08-09T18:08:28.005677Z",
     "shell.execute_reply": "2025-08-09T18:08:28.004941Z",
     "shell.execute_reply.started": "2025-08-09T18:08:27.926825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ STARTING DEPTH EXPANSION for video: 0074f737-11cb-497d-8d07-77c3a8127391\n",
      "============================================================\n",
      "ğŸ“‚ Loading features for 0074f737-11cb-497d-8d07-77c3a8127391...\n",
      "âœ… Features loaded: shape torch.Size([180, 1024])\n",
      "ğŸ“Š Processing relevance scores...\n",
      "ğŸ’¾ SAVING DEPTH EXPANSION RESULTS\n",
      "==================================================\n",
      "ğŸ“ Output path: ./outputs/depth_expansion_0074f737-11cb-497d-8d07-77c3a8127391.json\n",
      "ğŸ“ File size: 5.12 KB\n",
      "âœ… Results saved successfully!\n",
      "\n",
      "ğŸ“Š DEPTH EXPANSION ANALYSIS\n",
      "==================================================\n",
      "ğŸ¬ Video: 0074f737-11cb-497d-8d07-77c3a8127391\n",
      "ğŸ“Š Total frames: 180\n",
      "ğŸ”¢ Feature dimensions: 1024\n",
      "\n",
      "ğŸ“ˆ Expansion Results:\n",
      "  - Original clusters: 8\n",
      "  - Original representative frames: 8\n",
      "  - New representative frames: 86\n",
      "  - Expansion ratio: 10.75x\n",
      "\n",
      "â­ Relevance Distribution:\n",
      "  - Score 1 (Low): 1 clusters\n",
      "  - Score 2 (Medium): 1 clusters\n",
      "  - Score 3 (High): 6 clusters\n",
      "\n",
      "ğŸ¯ Frame Coverage:\n",
      "  - Frame range: 0 â†’ 177\n",
      "  - Frame span: 178 frames\n",
      "\n",
      "ğŸ“ Representative Frames:\n",
      "  Original: [7, 20, 54, 87, 126, 141, 152, 169]\n",
      "  Expanded: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 20, 20, 22, 30, 31, 34, 40, 41, 53, 56, 62, 63, 64, 65, 67, 68, 70, 72, 74, 75, 79, 80, 83, 85, 87, 87, 88, 89, 90, 91, 92, 93, 102, 105, 107, 111, 114, 115, 116, 117, 118, 119, 121, 122, 124, 126, 126, 127, 128, 131, 140, 141, 141, 149, 151, 152, 153, 154, 155, 155, 156, 158, 160, 161, 162, 165, 168, 169, 169, 174, 175, 177]\n",
      "==================================================\n",
      "ğŸ‰ DEPTH EXPANSION COMPLETE!\n",
      "\n",
      "ğŸ“‹ SUMMARY:\n",
      "âœ… Successfully expanded from 8 to 86 frames\n",
      "ğŸ“ˆ 10.8x more detailed frame selection!\n",
      "ğŸ¬ Your video now has 86 key representative frames!\n",
      "\n",
      "============================================================\n",
      "ğŸš€ STARTING DEPTH EXPANSION for video: 00b9a0de-c59e-49cb-a127-6081e2fb8c8e\n",
      "============================================================\n",
      "ğŸ“‚ Loading features for 00b9a0de-c59e-49cb-a127-6081e2fb8c8e...\n",
      "âœ… Features loaded: shape torch.Size([180, 1024])\n",
      "ğŸ“Š Processing relevance scores...\n",
      "ğŸ’¾ SAVING DEPTH EXPANSION RESULTS\n",
      "==================================================\n",
      "ğŸ“ Output path: ./outputs/depth_expansion_00b9a0de-c59e-49cb-a127-6081e2fb8c8e.json\n",
      "ğŸ“ File size: 5.79 KB\n",
      "âœ… Results saved successfully!\n",
      "\n",
      "ğŸ“Š DEPTH EXPANSION ANALYSIS\n",
      "==================================================\n",
      "ğŸ¬ Video: 00b9a0de-c59e-49cb-a127-6081e2fb8c8e\n",
      "ğŸ“Š Total frames: 180\n",
      "ğŸ”¢ Feature dimensions: 1024\n",
      "\n",
      "ğŸ“ˆ Expansion Results:\n",
      "  - Original clusters: 32\n",
      "  - Original representative frames: 32\n",
      "  - New representative frames: 98\n",
      "  - Expansion ratio: 3.06x\n",
      "\n",
      "â­ Relevance Distribution:\n",
      "  - Score 1 (Low): 5 clusters\n",
      "  - Score 2 (Medium): 0 clusters\n",
      "  - Score 3 (High): 0 clusters\n",
      "\n",
      "ğŸ¯ Frame Coverage:\n",
      "  - Frame range: 1 â†’ 179\n",
      "  - Frame span: 179 frames\n",
      "\n",
      "ğŸ“ Representative Frames:\n",
      "  Original: [5, 7, 15, 17, 20, 21, 27, 31, 41, 47, 56, 59, 62, 74, 77, 98, 100, 103, 110, 114, 117, 118, 122, 130, 141, 153, 156, 159, 163, 167, 171, 179]\n",
      "  Expanded: [1, 2, 3, 3, 4, 5, 6, 10, 11, 12, 13, 14, 17, 17, 18, 19, 21, 23, 24, 25, 26, 30, 31, 34, 46, 48, 49, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 66, 68, 71, 72, 73, 87, 88, 91, 96, 97, 98, 103, 105, 106, 107, 108, 110, 114, 114, 115, 121, 122, 122, 123, 124, 130, 130, 133, 136, 139, 141, 142, 144, 146, 147, 148, 151, 152, 153, 154, 154, 155, 156, 156, 158, 159, 159, 160, 161, 162, 163, 163, 164, 167, 170, 171, 171, 174, 176, 177, 179]\n",
      "==================================================\n",
      "ğŸ‰ DEPTH EXPANSION COMPLETE!\n",
      "\n",
      "ğŸ“‹ SUMMARY:\n",
      "âœ… Successfully expanded from 32 to 98 frames\n",
      "ğŸ“ˆ 3.1x more detailed frame selection!\n",
      "ğŸ¬ Your video now has 98 key representative frames!\n"
     ]
    }
   ],
   "source": [
    "for vid in video_ids:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸš€ STARTING DEPTH EXPANSION for video: {vid}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    groq_path = f'./outputs/video_{vid}_groq_pipeline.json'\n",
    "    config = DepthExpansionConfig(vid, groq_path)\n",
    "    config.output_filename = f'depth_expansion_{vid}.json'  # unique output filename\n",
    "    \n",
    "    # -- Cell 2 part to load groq results (assumed you did this outside or add here) --\n",
    "    try:\n",
    "        with open(config.groq_results_path, 'r') as f:\n",
    "            groq_results = json.load(f)\n",
    "        cluster_assignments = groq_results['final_result']['cluster_assignments']\n",
    "        frame_relevance = groq_results['final_result']['frame_relevance'] \n",
    "        representative_frames = groq_results['final_result']['representative_frames']\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Groq results for {vid}: {e}\")\n",
    "        continue  # skip to next video\n",
    "    \n",
    "    # -- Cell 3: Run Depth Expansion --\n",
    "    print(f\"ğŸ“‚ Loading features for {config.video_id}...\")\n",
    "    img_feats = load_image_features(config.video_id, config.save_folder)\n",
    "    img_feats = img_feats.cpu()\n",
    "    print(f\"âœ… Features loaded: shape {img_feats.shape}\")\n",
    "\n",
    "    print(f\"ğŸ“Š Processing relevance scores...\")\n",
    "    if isinstance(frame_relevance, list) and len(frame_relevance) > 0:\n",
    "        relevance_scores = frame_relevance\n",
    "    else:\n",
    "        num_clusters = len(set(cluster_assignments))\n",
    "        relevance_scores = [2] * num_clusters\n",
    "\n",
    "    clusters_info = hierarchical_clustering_with_external_primary(\n",
    "        img_feats, \n",
    "        cluster_assignments, \n",
    "        relevance_scores,\n",
    "        num_subclusters=config.num_subclusters,\n",
    "        num_subsubclusters=config.num_subsubclusters\n",
    "    )\n",
    "\n",
    "    closest_points_temporal = find_closest_points_in_temporal_order_subsub(\n",
    "        img_feats, \n",
    "        clusters_info, \n",
    "        relevance_scores\n",
    "    )\n",
    "    \n",
    "    # -- Cell 4: Save Results and Analysis --\n",
    "    depth_results = {\n",
    "        \"video_id\": config.video_id,\n",
    "        \"input_data\": {\n",
    "            \"total_frames\": len(img_feats),\n",
    "            \"feature_dimensions\": img_feats.shape[1] if len(img_feats.shape) > 1 else 1,\n",
    "            \"original_clusters\": len(set(cluster_assignments)),\n",
    "            \"original_representative_frames\": representative_frames,\n",
    "            \"relevance_scores\": relevance_scores,\n",
    "            \"cluster_assignments\": cluster_assignments\n",
    "        },\n",
    "        \"depth_expansion_config\": {\n",
    "            \"num_subclusters\": config.num_subclusters,\n",
    "            \"num_subsubclusters\": config.num_subsubclusters\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"hierarchical_clusters\": len(clusters_info),\n",
    "            \"final_representative_frames\": closest_points_temporal,\n",
    "            \"expansion_ratio\": len(closest_points_temporal) / len(representative_frames) if len(representative_frames) > 0 else 0,\n",
    "            \"total_representative_frames\": len(closest_points_temporal)\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"relevance_distribution\": {\n",
    "                \"score_1\": relevance_scores.count(1) if isinstance(relevance_scores, list) else 0,\n",
    "                \"score_2\": relevance_scores.count(2) if isinstance(relevance_scores, list) else 0,\n",
    "                \"score_3\": relevance_scores.count(3) if isinstance(relevance_scores, list) else 0\n",
    "            },\n",
    "            \"frame_coverage\": {\n",
    "                \"min_frame\": min(closest_points_temporal) if closest_points_temporal else 0,\n",
    "                \"max_frame\": max(closest_points_temporal) if closest_points_temporal else 0,\n",
    "                \"frame_span\": max(closest_points_temporal) - min(closest_points_temporal) + 1 if closest_points_temporal else 0\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    output_path = os.path.join(config.output_base_path, config.output_filename)\n",
    "    save_json(depth_results, output_path)\n",
    "\n",
    "    print(\"ğŸ’¾ SAVING DEPTH EXPANSION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ğŸ“ Output path: {output_path}\")\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        file_size = os.path.getsize(output_path) / 1024\n",
    "        print(f\"ğŸ“ File size: {file_size:.2f} KB\")\n",
    "    print(f\"âœ… Results saved successfully!\")\n",
    "\n",
    "    # Print detailed analysis summary\n",
    "    print(f\"\\nğŸ“Š DEPTH EXPANSION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    results = depth_results[\"results\"]\n",
    "    analysis = depth_results[\"analysis\"]\n",
    "\n",
    "    print(f\"ğŸ¬ Video: {depth_results['video_id']}\")\n",
    "    print(f\"ğŸ“Š Total frames: {depth_results['input_data']['total_frames']}\")\n",
    "    print(f\"ğŸ”¢ Feature dimensions: {depth_results['input_data']['feature_dimensions']}\")\n",
    "\n",
    "    print(f\"\\nğŸ“ˆ Expansion Results:\")\n",
    "    print(f\"  - Original clusters: {depth_results['input_data']['original_clusters']}\")\n",
    "    print(f\"  - Original representative frames: {len(depth_results['input_data']['original_representative_frames'])}\")\n",
    "    print(f\"  - New representative frames: {results['total_representative_frames']}\")\n",
    "    print(f\"  - Expansion ratio: {results['expansion_ratio']:.2f}x\")\n",
    "\n",
    "    print(f\"\\nâ­ Relevance Distribution:\")\n",
    "    rel_dist = analysis[\"relevance_distribution\"]\n",
    "    print(f\"  - Score 1 (Low): {rel_dist['score_1']} clusters\")\n",
    "    print(f\"  - Score 2 (Medium): {rel_dist['score_2']} clusters\")\n",
    "    print(f\"  - Score 3 (High): {rel_dist['score_3']} clusters\")\n",
    "\n",
    "    print(f\"\\nğŸ¯ Frame Coverage:\")\n",
    "    coverage = analysis[\"frame_coverage\"]\n",
    "    print(f\"  - Frame range: {coverage['min_frame']} â†’ {coverage['max_frame']}\")\n",
    "    print(f\"  - Frame span: {coverage['frame_span']} frames\")\n",
    "\n",
    "    print(f\"\\nğŸ“ Representative Frames:\")\n",
    "    print(f\"  Original: {depth_results['input_data']['original_representative_frames']}\")\n",
    "    print(f\"  Expanded: {results['final_representative_frames']}\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(\"ğŸ‰ DEPTH EXPANSION COMPLETE!\")\n",
    "\n",
    "    print(f\"\\nğŸ“‹ SUMMARY:\")\n",
    "    if results['expansion_ratio'] > 1:\n",
    "        print(f\"âœ… Successfully expanded from {len(representative_frames)} to {results['total_representative_frames']} frames\")\n",
    "        print(f\"ğŸ“ˆ {results['expansion_ratio']:.1f}x more detailed frame selection!\")\n",
    "    elif results['expansion_ratio'] == 1:\n",
    "        print(f\"â¡ï¸  Same number of frames, but hierarchically organized\")\n",
    "    else:\n",
    "        print(f\"ğŸ“‰ Fewer frames selected: {results['total_representative_frames']} vs {len(representative_frames)}\")\n",
    "\n",
    "    print(f\"ğŸ¬ Your video now has {results['total_representative_frames']} key representative frames!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:08:35.621244Z",
     "iopub.status.busy": "2025-08-09T18:08:35.620977Z",
     "iopub.status.idle": "2025-08-09T18:08:51.578847Z",
     "shell.execute_reply": "2025-08-09T18:08:51.578249Z",
     "shell.execute_reply.started": "2025-08-09T18:08:35.621226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 18:08:38.111926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754762918.333670      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754762918.399070      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# -------- Globals --------\n",
    "vl_model = None\n",
    "vl_processor = None\n",
    "vl_device = None\n",
    "\n",
    "VL_MODEL_ID = \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\"\n",
    "\n",
    "def ensure_vl_model_loaded(\n",
    "    model_id: str = VL_MODEL_ID,\n",
    "    gpu_index: int = 0,\n",
    "    device_map: str | None = \"auto\",  # \"auto\" spreads layers, None keeps manual device handling\n",
    "    torch_dtype: torch.dtype | None = None,  # Keep None for 4-bit; HF sets dtype appropriately\n",
    ") -> tuple[AutoModelForVision2Seq, AutoProcessor, str]:\n",
    "    \"\"\"\n",
    "    Loads Qwen2.5-VL once and returns (model, processor, device).\n",
    "    Subsequent calls return the already-loaded singleton.\n",
    "    \"\"\"\n",
    "    global vl_model, vl_processor, vl_device\n",
    "    if vl_model is not None and vl_processor is not None and vl_device is not None:\n",
    "        return vl_model, vl_processor, vl_device\n",
    "\n",
    "    # Resolve target device\n",
    "    if torch.cuda.is_available():\n",
    "        vl_device = f\"cuda:{gpu_index}\"\n",
    "    else:\n",
    "        vl_device = \"cpu\"\n",
    "\n",
    "    # Load processor\n",
    "    vl_processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # Load model (4-bit quantized). Prefer device_map=\"auto\" for correct placement.\n",
    "    load_kwargs = dict(trust_remote_code=True, low_cpu_mem_usage=True)\n",
    "    if device_map is not None:\n",
    "        load_kwargs[\"device_map\"] = device_map\n",
    "    if torch_dtype is not None:\n",
    "        load_kwargs[\"torch_dtype\"] = torch_dtype\n",
    "\n",
    "    vl_model = AutoModelForVision2Seq.from_pretrained(model_id, **load_kwargs)\n",
    "    vl_model.eval()\n",
    "\n",
    "    # If not using device_map, move the whole model to the resolved device\n",
    "    if device_map in (None, \"none\"):\n",
    "        vl_model = vl_model.to(vl_device)\n",
    "\n",
    "    return vl_model, vl_processor, vl_device\n",
    "\n",
    "def unload_vl_model():\n",
    "    \"\"\"Optional: free the singleton to reclaim memory.\"\"\"\n",
    "    global vl_model, vl_processor, vl_device\n",
    "    vl_model = None\n",
    "    vl_processor = None\n",
    "    vl_device = None\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:09:35.475754Z",
     "iopub.status.busy": "2025-08-09T18:09:35.475030Z",
     "iopub.status.idle": "2025-08-09T18:10:25.873830Z",
     "shell.execute_reply": "2025-08-09T18:10:25.872985Z",
     "shell.execute_reply.started": "2025-08-09T18:09:35.475729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b909d56f6645ac92100ed0f334bd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd798c1bd35d4df88cb888333612dfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077c79525c56481580c42509ece3c720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08817d6c5d6042ccbd0aa891f3326617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047d8495f8224b4cb004dc814ca3ab78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c68f8fb0bb4c649c853c942f1cd7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e888c26d4c64814a7c94c469924a853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc3c4753d184f0bb8320da03053761f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a449eb4fd5c40c694c0c06d67b6d47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/modeling_auto.py:2199: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a5de641cb949e79be93445b6e87730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9108638ec19f4fdba0a09581d498dcdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ab2d759ff34fe9a4f15a804fb49130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, processor, device = ensure_vl_model_loaded(\n",
    "    model_id=\"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n",
    "    gpu_index=0,          # set to 1 if you intend to use cuda:1\n",
    "    device_map=\"auto\"     # or None to force .to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T18:10:58.364618Z",
     "iopub.status.busy": "2025-08-09T18:10:58.364062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Important frames from JSON: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 20, 20, 22, 30, 31, 34, 40, 41, 53, 56, 62, 63, 64, 65, 67, 68, 70, 72, 74, 75, 79, 80, 83, 85, 87, 87, 88, 89, 90, 91, 92, 93, 102, 105, 107, 111, 114, 115, 116, 117, 118, 119, 121, 122, 124, 126, 126, 127, 128, 131, 140, 141, 141, 149, 151, 152, 153, 154, 155, 155, 156, 158, 160, 161, 162, 165, 168, 169, 169, 174, 175, 177]\n",
      "ğŸï¸ Video FPS: 30.0, total frames: 5400, duration: 180.00s\n",
      "âœ… Extracted 180 frames at ~1 FPS\n",
      "ğŸ–¼ï¸ Found 86 important frames\n",
      "ğŸ”„ Running batch 1 with 10 images\n",
      "ğŸ”„ Running batch 2 with 10 images\n",
      "ğŸ”„ Running batch 3 with 10 images\n",
      "ğŸ”„ Running batch 4 with 10 images\n",
      "ğŸ”„ Running batch 5 with 10 images\n",
      "ğŸ”„ Running batch 6 with 10 images\n",
      "ğŸ”„ Running batch 7 with 10 images\n",
      "ğŸ”„ Running batch 8 with 10 images\n",
      "ğŸ”„ Running batch 9 with 6 images\n",
      "\n",
      "ğŸ¥ Video ID: 0074f737-11cb-497d-8d07-77c3a8127391\n",
      "â“ Question: Question:\n",
      "Taking into account all the actions performed by c, what can you deduce about the primary objective and focus within the video content?\n",
      "\n",
      "Options:\n",
      "A. C is cooking.\n",
      "B. C is doing laundry.\n",
      "C. C is cleaning the kitchen.\n",
      "D. C is cleaning dishes.\n",
      "E. C is cleaning the bathroom.\n",
      "\n",
      "Please choose the most appropriate answer (Aâ€“E).\n",
      "\n",
      "=== ğŸ§  Answers ===\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "D. C is cleaning dishes.<|im_end|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ğŸ“Œ Important frames from JSON: [1, 2, 3, 3, 4, 5, 6, 10, 11, 12, 13, 14, 17, 17, 18, 19, 21, 23, 24, 25, 26, 30, 31, 34, 46, 48, 49, 51, 52, 54, 55, 56, 57, 60, 61, 62, 63, 66, 68, 71, 72, 73, 87, 88, 91, 96, 97, 98, 103, 105, 106, 107, 108, 110, 114, 114, 115, 121, 122, 122, 123, 124, 130, 130, 133, 136, 139, 141, 142, 144, 146, 147, 148, 151, 152, 153, 154, 154, 155, 156, 156, 158, 159, 159, 160, 161, 162, 163, 163, 164, 167, 170, 171, 171, 174, 176, 177, 179]\n",
      "ğŸï¸ Video FPS: 30.0, total frames: 5400, duration: 180.00s\n",
      "âœ… Extracted 180 frames at ~1 FPS\n",
      "ğŸ–¼ï¸ Found 98 important frames\n",
      "ğŸ”„ Running batch 1 with 10 images\n"
     ]
    }
   ],
   "source": [
    "for video_id in video_ids:\n",
    "\n",
    "    video_path = f\"/kaggle/working/downloaded_videos/{video_id}.mp4\"\n",
    "    json_path = f'/kaggle/working/outputs/depth_expansion_{video_id}.json'\n",
    "    \n",
    "    question = questions[video_id]\n",
    "    \n",
    "    # Step 1: Extract frames at 1 FPS\n",
    "    def extract_frames_at_1fps(video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration_sec = total_frames / fps\n",
    "        print(f\"ğŸï¸ Video FPS: {fps}, total frames: {total_frames}, duration: {duration_sec:.2f}s\")\n",
    "    \n",
    "        frames = {}\n",
    "        frame_idx = 0\n",
    "        saved_idx = 0\n",
    "    \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_idx % int(fps) == 0:\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames[saved_idx] = Image.fromarray(rgb)\n",
    "                saved_idx += 1\n",
    "            frame_idx += 1\n",
    "    \n",
    "        cap.release()\n",
    "        print(f\"âœ… Extracted {saved_idx} frames at ~1 FPS\")\n",
    "        return frames\n",
    "    \n",
    "    # Step 2: Load important frame indices from JSON\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    important_indices = data['results']['final_representative_frames']\n",
    "    print(f\"ğŸ“Œ Important frames from JSON: {important_indices}\")\n",
    "    \n",
    "    # Step 3: Match with extracted frames\n",
    "    all_frames = extract_frames_at_1fps(video_path)\n",
    "    important_images = []\n",
    "    missing = []\n",
    "    \n",
    "    for idx in important_indices:\n",
    "        if idx in all_frames:\n",
    "            important_images.append(all_frames[idx])\n",
    "        else:\n",
    "            missing.append(idx)\n",
    "    \n",
    "    print(f\"ğŸ–¼ï¸ Found {len(important_images)} important frames\")\n",
    "    if missing:\n",
    "        print(f\"âš ï¸ Missing frames (not in 1FPS output): {missing}\")\n",
    "    \n",
    "    # Step 4: Helper - batching\n",
    "    def chunk_list(lst, size):\n",
    "        for i in range(0, len(lst), size):\n",
    "            yield lst[i:i + size]\n",
    "    \n",
    "    # Step 5: Run inference in batches\n",
    "    batch_size = 10  # tweak this if OOM persists\n",
    "    answers = []\n",
    "    \n",
    "    for i, batch in enumerate(chunk_list(important_images, batch_size)):\n",
    "        print(f\"ğŸ”„ Running batch {i+1} with {len(batch)} images\")\n",
    "    \n",
    "        content = [{\"type\": \"image\", \"image\": img} for img in batch]\n",
    "        content.append({\"type\": \"text\", \"text\": question})\n",
    "        messages = [{\"role\": \"user\", \"content\": content}]\n",
    "    \n",
    "        try:\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "    \n",
    "            outputs = model.generate(**inputs, max_new_tokens=50)  # Reduce tokens for memory\n",
    "            answer = processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "            answers.append(answer.strip())\n",
    "    \n",
    "            torch.cuda.empty_cache()  # Free up memory\n",
    "    \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(\"âŒ OOM in batch. Skipping or lowering batch size.\")\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "\n",
    "    \n",
    "    # Step 6: Combine and print final answer\n",
    "    final_answer = \"\\n\".join(answers)\n",
    "\n",
    "    print(f\"\\nğŸ¥ Video ID: {video_id}\")\n",
    "    print(f\"â“ Question: {question}\")\n",
    "    print(\"\\n=== ğŸ§  Answers ===\")\n",
    "    print(final_answer)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7997229,
     "sourceId": 12683551,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
