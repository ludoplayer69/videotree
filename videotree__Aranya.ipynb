{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12654575,"sourceType":"datasetVersion","datasetId":7997229}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Dependencies","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/subhadarship/kmeans_pytorch\n%cd kmeans_pytorch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --editable .\n!pip install --upgrade transformers accelerate bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Copying 1 Video","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\nfrom pathlib import Path\n\nsource = '/kaggle/input/egoschema/fed08b9b-7cbf-4f96-86a0-567a96b80125.mp4'\n\ndestination_folder = Path('/kaggle/working/videos')\ndestination_folder.mkdir(parents=True, exist_ok=True)\n\n# This will copy the file with the same name\nshutil.copy(source, os.path.join(destination_folder, os.path.basename(source)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Extract Images","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport json\n\ndef load_json(fn):\n    with open(fn, 'r') as f:\n        data = json.load(f)\n    return data\n\ndef save_json(data, fn, indent=4):\n    with open(fn, 'w') as f:\n        json.dump(data, f, indent = indent)\n\ninput_base_path = Path('/kaggle/working/videos')\noutput_base_path = Path('/kaggle/working/extracted_frames')\noutput_base_path.mkdir(parents=True, exist_ok=True)\n\nfps = 1\n\npbar = tqdm(total = len(list(input_base_path.iterdir())))\n\nfor video_fp in input_base_path.iterdir():\n    output_path = output_base_path / video_fp.stem\n    output_path.mkdir(parents=True, exist_ok=True) # video_fp.stem refers to the filename without the extension.\n\n    vidcap = cv2.VideoCapture(str(video_fp)) # read videos frame by frame\n\n    count = 0\n\n    success = True\n\n    fps_ori = int(vidcap.get(cv2.CAP_PROP_FPS)) # retrieves the original FPS of the input video\n\n    frame_interval = int(fps_ori / fps)\n\n    while success:\n        success, image = vidcap.read()\n\n        if not success:\n            break\n        if count % frame_interval == 0:\n            cv2.imwrite(f'{output_path}/{count}.jpg', image) # save frame as JPG file\n        count += 1\n    pbar.update(1)\npbar.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Extract Features","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import CLIPImageProcessor, AutoModel, BitsAndBytesConfig\n\n# ----------- Configuration -----------\nMODEL_NAME = \"BAAI/EVA-CLIP-8B\"\nIMAGE_SIZE = 224\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nBASE_PATH = Path('/kaggle/working/extracted_frames')\nSAVE_PATH = Path('/kaggle/working/extracted_features')\nSAVE_PATH.mkdir(parents=True, exist_ok=True)\n\nJSON_PATH = Path('/kaggle/input/egoschema/fullset_anno.json')\nMAX_EXAMPLES = 50  # Set limit on number of examples to process\nRESUME = True  # Unused, but kept if resuming logic is added later\n\n# ----------- Load model and processor -----------\nprocessor = CLIPImageProcessor.from_pretrained('openai/clip-vit-large-patch14')\nquant_config = BitsAndBytesConfig(load_in_4bit=True)\n\nmodel = AutoModel.from_pretrained(\n    MODEL_NAME,\n    trust_remote_code=True,\n    quantization_config=quant_config\n).to(DEVICE).eval()\n\n# ----------- Utility Functions -----------\n\ndef save_image_features(img_feats: torch.Tensor, name_id: str, save_folder: Path):\n    \"\"\"Saves extracted image features to disk.\"\"\"\n    torch.save(img_feats, save_folder / f\"{name_id}.pt\")\n\ndef load_json(file_path: Path):\n    with open(file_path, 'r') as f:\n        return json.load(f)\n\n# ----------- Main Processing Loop -----------\n\ndef process_images():\n    json_data = load_json(JSON_PATH)\n    valid_names = set(json_data.keys())\n\n    example_dirs = list(BASE_PATH.iterdir())\n    processed = 0\n\n    pbar = tqdm(total=min(len(example_dirs), MAX_EXAMPLES), desc=\"Processing sets\")\n\n    for example_dir in example_dirs:\n        if processed >= MAX_EXAMPLES:\n            break\n        if example_dir.name not in valid_names:\n            continue\n\n        image_files = sorted(example_dir.iterdir(), key=lambda x: int(x.stem))\n        feature_list = []\n\n        for image_file in image_files:\n            image = Image.open(image_file)\n            inputs = processor(images=image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n\n            with torch.no_grad(), torch.cuda.amp.autocast():\n                feats = model.encode_image(inputs)\n                feature_list.append(feats)\n\n        stacked_feats = torch.stack(feature_list).squeeze(1)\n        save_image_features(stacked_feats, example_dir.name, SAVE_PATH)\n\n        processed += 1\n        pbar.update(1)\n\n    pbar.close()\n\n# ----------- Run -----------\nif __name__ == \"__main__\":\n    process_images()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Adaptive Breath Expansion","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/Ziyang412/VideoTree.git \n%cd /kaggle/working/VideoTree","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install groq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"GROQ_API_KEY\"] = \"Put Your Key Here\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Imports and Configuration\nimport os\nimport json\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom pprint import pprint\nfrom kmeans_pytorch import kmeans\nimport torch\nimport re\nfrom groq import Groq\n\n# Import your original modules\nfrom util import *\nfrom prompts import PromptFactory\n\nclass SingleVideoConfig:\n    \"\"\"Configuration for single video processing with Groq\"\"\"\n    def __init__(self):\n        # Paths - UPDATE THESE FOR YOUR SETUP\n        self.output_base_path = './outputs'\n        self.output_filename = 'single_video_groq_pipeline.json'\n        \n        # Single video settings\n        self.feature_file_path = '/kaggle/working/extracted_features/fed08b9b-7cbf-4f96-86a0-567a96b80125.pt'\n        self.video_id = 'fed08b9b-7cbf-4f96-86a0-567a96b80125'\n        \n        # Original pipeline parameters\n        self.frame_feat_path = '/kaggle/working/extracted_features'\n        self.start_from_scratch = True\n        self.num_examples_to_run = 1  # Just one video\n        self.prompt_type = 'cap_score'\n        self.fewshot_example_path = '/kaggle/input/egoschema/few_shot_6.json'\n        self.data_path = '/kaggle/input/egoschema/fullset_anno.json'  # Keep for prompt setup\n        self.max_cluster_num = 32\n        self.init_cluster_num = 4\n        self.iter_threshold = 5\n        self.default_adaptive_rate = 2\n        self.fps = 1.0\n        self.num_words_in_sum = 100\n        self.save_info = True\n        self.save_every = 1\n        self.backup_pred_path = ''\n        self.disable_eval = True\n        self.task = 'sum'\n        self.dataset = 'egoschema'\n        self.anno_path = ''\n        self.captions_file_path = '/kaggle/input/egoschema/blip2_fullset.json'\n        \n        # Groq model configuration\n        self.model = 'llama-3.3-70b-versatile'\n        self.temperature = 0.0\n        self.max_tokens = 1000\n\n# Initialize config\nconfig = SingleVideoConfig()\nprint(\"Configuration loaded!\")\npprint(vars(config))\n\n\n# Cell 2: Initialize Groq Model\nfrom groq import Groq\n\nclass GroqModel:\n    \"\"\"Wrapper for Groq API to match original model interface\"\"\"\n    def __init__(self, model_name='llama-3.3-70b-versatile', temperature=0.0, max_tokens=1000):\n        self.client = Groq()  # Uses GROQ_API_KEY environment variable\n        self.model_name = model_name\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.post_process_fn = None\n        \n    def set_post_process_fn(self, fn):\n        \"\"\"Set post-processing function (same interface as original)\"\"\"\n        self.post_process_fn = fn\n        \n    def forward(self, system_prompt, user_prompt):\n        \"\"\"Forward pass - same interface as original model\"\"\"\n        try:\n            # Combine prompts if user_prompt is a list\n            if isinstance(user_prompt, list):\n                combined_prompt = \"\\n\".join(user_prompt)\n            else:\n                combined_prompt = user_prompt\n                \n            # Make API call\n            chat_completion = self.client.chat.completions.create(\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": combined_prompt}\n                ],\n                model=self.model_name,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens\n            )\n            \n            # Get response\n            response = chat_completion.choices[0].message.content\n            \n            # Apply post-processing if available\n            if self.post_process_fn:\n                pred = self.post_process_fn(response)\n            else:\n                pred = response\n                \n            # Return in same format as original\n            info = {\n                'response': response,\n                'model': self.model_name,\n                'tokens_used': chat_completion.usage.total_tokens if hasattr(chat_completion, 'usage') else 0\n            }\n            \n            return pred, info\n            \n        except Exception as e:\n            print(f\"Error in Groq API call: {e}\")\n            return [], {'response': '', 'error': str(e)}\n\n# Initialize Groq model\nprint(\"Initializing Groq model...\")\nmodel = GroqModel(\n    model_name=config.model,\n    temperature=config.temperature,\n    max_tokens=config.max_tokens\n)\nprint(f\"âœ… Groq model initialized: {config.model}\")\n\n# Test the model with a simple call\ntry:\n    test_pred, test_info = model.forward(\n        \"You are a helpful assistant.\", \n        \"Say 'Hello, I am ready!' if you can understand this.\"\n    )\n    print(f\"âœ… Model test successful: {test_info['response'][:50]}...\")\nexcept Exception as e:\n    print(f\"âŒ Model test failed: {e}\")\n    print(\"Please check your GROQ_API_KEY environment variable\")\n\n# Cell 3: Utility Functions\n\ndef load_frame_captions(captions_file_path, video_id):\n    \"\"\"Load frame captions for a specific video from JSON file\"\"\"\n    try:\n        captions_data = load_json(captions_file_path)\n        \n        if video_id in captions_data:\n            video_captions_raw = captions_data[video_id]\n            \n            # Handle if it's a list instead of dict\n            if isinstance(video_captions_raw, list):\n                print(f\"âœ… Found {len(video_captions_raw)} frame captions (list format)\")\n                # Convert list to dict with frame indices\n                frame_captions = {}\n                for i, caption in enumerate(video_captions_raw):\n                    frame_captions[i] = caption\n                return frame_captions\n            else:\n                # Handle dict format\n                print(f\"âœ… Found frame captions (dict format)\")\n                frame_captions = {}\n                for key, caption in video_captions_raw.items():\n                    try:\n                        frame_idx = int(key)\n                        frame_captions[frame_idx] = caption\n                    except ValueError:\n                        continue\n                return frame_captions\n        else:\n            print(f\"âŒ Video {video_id} not found in captions file\")\n            return {}\n            \n    except Exception as e:\n        print(f\"âŒ Error loading captions: {e}\")\n        return {}\n\ndef get_caption_for_frame(frame_idx, video_captions):\n    \"\"\"Get caption for a specific frame index\"\"\"\n    if frame_idx in video_captions:\n        return video_captions[frame_idx]\n    else:\n        # Return None for missing frames - we'll handle this properly\n        return None\n\ndef load_frame_features(video_id, features_folder):\n    \"\"\"Load frame features for a video\"\"\"\n    filename = f\"{video_id}.pt\"\n    filepath = os.path.join(features_folder, filename)\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"Feature file not found: {filepath}\")\n    return torch.load(filepath)\n\ndef find_closest_points_per_cluster(features, cluster_ids, cluster_centers):\n    \"\"\"Find closest points to cluster centers\"\"\"\n    closest_points_idx_per_cluster = {cluster_id: [] for cluster_id in range(len(cluster_centers))}\n    \n    for cluster_id in range(len(cluster_centers)):\n        indices_in_cluster = torch.where(cluster_ids == cluster_id)[0]\n        points_in_cluster = features[indices_in_cluster]\n        distances = torch.norm(points_in_cluster - cluster_centers[cluster_id], dim=1)\n        if distances.numel() > 0:\n            closest_idx_in_cluster = torch.argmin(distances).item()\n            closest_global_idx = indices_in_cluster[closest_idx_in_cluster].item()\n            closest_points_idx_per_cluster[cluster_id].append(closest_global_idx)\n    \n    return closest_points_idx_per_cluster\n\ndef build_fewshot_examples(fewshot_path, data_path):\n    \"\"\"Build few-shot examples if available\"\"\"\n    try:\n        if os.path.exists(fewshot_path):\n            return load_json(fewshot_path)\n        else:\n            print(f\"Few-shot file not found: {fewshot_path}, using empty examples\")\n            return []\n    except:\n        print(\"Error loading few-shot examples, using empty list\")\n        return []\n\ndef create_dummy_item(video_id, config):\n    \"\"\"Create a dummy item structure for the single video\"\"\"\n    return {\n        'quid': video_id,\n        'uid': video_id,\n        'video_id': video_id,\n        'question': 'What activities does the person perform in this video?',\n        'options': ['A) Cooking', 'B) Cleaning', 'C) Reading', 'D) Working', 'E) Other'],\n        'optionA': 'Cooking and food preparation',\n        'optionB': 'Cleaning and organizing',\n        'optionC': 'Reading and studying', \n        'optionD': 'Working on computer',\n        'optionE': 'Other activities',\n        'answer': 'A',\n        'duration': 180,  # Will be updated with actual frame count\n        'narration': 'Video frame descriptions will be inserted here by the clustering algorithm'\n    }\n\ndef create_relevance_prompt(tree_node, video_captions, item):\n    \"\"\"Create a focused prompt for relevance scoring\"\"\"\n    \n    # Get captions for representative frames (only those that have captions)\n    frame_descriptions = []\n    valid_frames = []\n    \n    for frame_idx in tree_node:\n        caption = get_caption_for_frame(frame_idx, video_captions)\n        if caption:  # Only include frames with actual captions\n            frame_descriptions.append(f\"Frame {frame_idx}: {caption}\")\n            valid_frames.append(frame_idx)\n    \n    if not frame_descriptions:\n        print(\"âŒ No captions found for any representative frames!\")\n        return None, []\n    \n    print(f\"ğŸ“ Using {len(frame_descriptions)} frames with captions out of {len(tree_node)} total\")\n    \n    # Create focused prompt for relevance scoring\n    prompt = f\"\"\"VIDEO QUESTION: {item['question']}\n\nOPTIONS:\n{chr(10).join([f\"{opt}\" for opt in item['options']])}\n\nFRAME DESCRIPTIONS:\n{chr(10).join(frame_descriptions)}\n\nTASK: Rate each frame's relevance to answering the question on a scale of 1-3:\n- 1 = Not relevant (doesn't help answer the question)\n- 2 = Somewhat relevant (provides some context)  \n- 3 = Highly relevant (directly helps answer the question)\n\nProvide ONLY the relevance scores in this exact format:\nframe relevance: [score1, score2, score3, ...]\n\nYou must provide exactly {len(frame_descriptions)} scores for the {len(frame_descriptions)} frames described above.\n\nExample: frame relevance: [2, 1, 3, 2, 1]\"\"\"\n    \n    return prompt, valid_frames\n\ndef update_relevance_response(text):\n    \"\"\"Extract relevance scores from model response - IMPROVED VERSION\"\"\"\n    response = text.strip()\n    print(f\"ğŸ¤– Model response: {response}\")\n    \n    # Try multiple patterns to extract relevance scores\n    patterns = [\n        r\"frame relevance:\\s*\\[([0-9,\\s]+)\\]\",\n        r\"relevance:\\s*\\[([0-9,\\s]+)\\]\", \n        r\"scores:\\s*\\[([0-9,\\s]+)\\]\",\n        r\"relevance scores:\\s*\\[([0-9,\\s]+)\\]\",\n        r\"\\[([0-9,\\s]+)\\]\"  # Any list of numbers\n    ]\n    \n    for pattern in patterns:\n        relevance_match = re.search(pattern, response, re.IGNORECASE)\n        if relevance_match:\n            try:\n                # Extract and clean the numbers\n                numbers_str = relevance_match.group(1)\n                relevance = [int(x.strip()) for x in numbers_str.split(',') if x.strip().isdigit()]\n                \n                # Validate the scores (should be 1, 2, or 3)\n                relevance = [max(1, min(3, score)) for score in relevance]\n                \n                print(f\"âœ… Extracted relevance scores: {relevance}\")\n                return relevance\n            except Exception as e:\n                print(f\"âŒ Error parsing relevance: {e}\")\n                continue\n    \n    print(\"âŒ No relevance scores found in response\")\n    return []\n\n# Load frame captions for this video\nprint(f\"\\nğŸ“ Loading frame captions...\")\nvideo_captions = load_frame_captions(config.captions_file_path, config.video_id)\n\nif video_captions:\n    print(f\"ğŸ“Š Available frames: {min(video_captions.keys())}-{max(video_captions.keys())}\")\n    print(f\"ğŸ“ Sample caption: '{list(video_captions.values())[0]}'\")\nelse:\n    print(\"âŒ No captions loaded - pipeline will exit\")\n    exit()\n\nprint(\"âœ… Utility functions loaded!\")\n\n# Cell 4: Initialize Prompter\n\n# Initialize prompter\nprompter = PromptFactory().get(config.prompt_type)\nprint(f\"âœ… Initialized prompter: {config.prompt_type}\")\n\n# Set up the model's post-processing function\nmodel.set_post_process_fn(update_relevance_response)\nprint(\"âœ… Set post-processing function for relevance extraction\")\n\n# Create dummy item for testing\nitem = create_dummy_item(config.video_id, config)\nprint(f\"âœ… Created dummy item for video: {config.video_id}\")\n\n# Cell 5: Load Video Features\n\nprint(f\"Loading features from: {config.feature_file_path}\")\n\n# Check if feature file exists\nif not os.path.exists(config.feature_file_path):\n    print(f\"âŒ ERROR: Feature file not found: {config.feature_file_path}\")\n    exit()\nelse:\n    # Load frame features\n    frame_feats = load_frame_features(config.video_id, config.frame_feat_path)\n    print(f\"âœ… Loaded features: shape {frame_feats.shape}, dtype {frame_feats.dtype}\")\n    \n    # Move to appropriate device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    frame_feats = frame_feats.to(device)\n    print(f\"âœ… Moved features to device: {device}\")\n    \n    print(f\"\\nFeature summary:\")\n    print(f\"  - Total frames: {len(frame_feats)}\")\n    print(f\"  - Feature dimensions: {frame_feats.shape[1] if len(frame_feats.shape) > 1 else 1}\")\n    print(f\"  - Device: {frame_feats.device}\")\n    print(f\"  - Memory usage: {frame_feats.element_size() * frame_feats.nelement() / 1024 / 1024:.2f} MB\")\n\n# Cell 6: Adaptive Clustering with Groq Model\n\ndef adaptive_clustering_with_groq(frame_feats, config, model, prompter, item, video_captions):\n    \"\"\"\n    Perform adaptive clustering with Groq model for relevance prediction\n    \"\"\"\n    cluster_num = config.init_cluster_num\n    device = frame_feats.device\n    \n    print(f\"ğŸš€ Starting adaptive clustering with Groq model\")\n    print(f\"ğŸ“Š Feature tensor shape: {frame_feats.shape}\")\n    print(f\"ğŸ–¥ï¸  Device: {device}\")\n    \n    clustering_results = []\n    \n    while cluster_num <= config.max_cluster_num:\n        try:\n            print(f\"\\n{'='*60}\")\n            print(f\"ğŸ”„ CLUSTERING ATTEMPT: {cluster_num} clusters\")\n            print(f\"{'='*60}\")\n            \n            # Perform k-means clustering\n            cluster_ids_x, cluster_centers = kmeans(\n                X=frame_feats, \n                num_clusters=cluster_num, \n                distance='cosine', \n                device=device\n            )\n            \n            cluster_ids_x = cluster_ids_x.to(device)\n            cluster_centers = cluster_centers.to(device)\n            \n            # Find representative frames\n            closest_points_idx_per_cluster = find_closest_points_per_cluster(\n                frame_feats, cluster_ids_x, cluster_centers\n            )\n            \n            if not closest_points_idx_per_cluster:\n                print(f\"âŒ No valid clusters found, increasing to {cluster_num * config.default_adaptive_rate}\")\n                cluster_num *= config.default_adaptive_rate\n                continue\n            \n            # Get representative frame indices\n            tree_node = sorted([value for sublist in closest_points_idx_per_cluster.values() for value in sublist])\n            cluster_ids_list = cluster_ids_x.tolist()\n            \n            print(f\"ğŸ“ Representative frames: {tree_node}\")\n            print(f\"ğŸ“Š Unique clusters found: {len(set(cluster_ids_list))}\")\n            \n            # Create custom relevance prompt instead of using original prompter\n            prompt, valid_frames = create_relevance_prompt(tree_node, video_captions, item)\n            \n            if not prompt:\n                print(f\"âŒ No valid prompt created, increasing clusters\")\n                cluster_num *= config.default_adaptive_rate\n                continue\n            \n            print(f\"ğŸ“‹ Created relevance prompt for {len(valid_frames)} frames\")\n            \n            # Model inference with Groq\n            print(f\"ğŸ¤– Calling Groq model...\")\n            pred, info = model.forward(\n                \"You are an expert video analyst. Analyze frame descriptions and rate their relevance.\", \n                prompt\n            )\n            \n            print(f\"ğŸ“¤ Model response received\")\n            print(f\"ğŸ¯ Extracted prediction: {pred}\")\n            \n            # Extract frame relevance\n            frame_relevance = pred\n            \n            # Count high relevance frames (score = 3)\n            if isinstance(frame_relevance, list) and len(frame_relevance) == len(valid_frames):\n                high_relevance_frame_num = frame_relevance.count(3)\n                print(f\"ğŸ“ˆ Relevance scores: {frame_relevance}\")\n                print(f\"ğŸ¯ High relevance frames (score=3): {high_relevance_frame_num}\")\n            else:\n                high_relevance_frame_num = 0\n                print(f\"âš ï¸  Relevance extraction failed or mismatch. Expected {len(valid_frames)}, got {len(frame_relevance) if isinstance(frame_relevance, list) else 'non-list'}\")\n            \n            print(f\"ğŸšï¸  Threshold: {config.iter_threshold}\")\n            \n            # Store clustering result\n            clustering_result = {\n                'num_clusters': cluster_num,\n                'actual_clusters': len(set(cluster_ids_list)),\n                'representative_frames': tree_node,\n                'valid_frames_with_captions': valid_frames,\n                'cluster_assignments': cluster_ids_list,\n                'frame_relevance': frame_relevance,\n                'high_relevance_count': high_relevance_frame_num,\n                'prompt': prompt,\n                'model_response': info.get('response', ''),\n                'tokens_used': info.get('tokens_used', 0)\n            }\n            clustering_results.append(clustering_result)\n            \n            # Check stopping condition\n            if high_relevance_frame_num < config.iter_threshold:\n                if cluster_num < config.max_cluster_num:\n                    print(f\"ğŸ“‰ Not enough high-relevance frames ({high_relevance_frame_num} < {config.iter_threshold})\")\n                    next_cluster_num = cluster_num * config.default_adaptive_rate\n                    print(f\"ğŸ”„ Increasing clusters: {cluster_num} â†’ {next_cluster_num}\")\n                    cluster_num = next_cluster_num\n                else:\n                    print(f\"ğŸ›‘ Reached max clusters ({config.max_cluster_num}), stopping\")\n                    break\n            else:\n                print(f\"âœ… Found sufficient high-relevance frames ({high_relevance_frame_num} >= {config.iter_threshold})\")\n                print(f\"ğŸ Stopping clustering - SUCCESS!\")\n                break\n                \n        except Exception as e:\n            print(f\"âŒ Clustering failed with {cluster_num} clusters: {e}\")\n            import traceback\n            traceback.print_exc()\n            cluster_num *= config.default_adaptive_rate\n            continue\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"ğŸ† CLUSTERING COMPLETE!\")\n    print(f\"{'='*60}\")\n    \n    # Return the final successful clustering result\n    if clustering_results:\n        final_result = clustering_results[-1]\n        return (final_result['representative_frames'], \n                final_result['cluster_assignments'], \n                final_result['frame_relevance'],\n                final_result['high_relevance_count'],\n                clustering_results)\n    else:\n        return [], [], [], 0, []\n\nprint(\"âœ… Adaptive clustering function loaded!\")\n\n# Cell 7: Run the Complete Pipeline\n\nprint(\"ğŸš€ STARTING COMPLETE PIPELINE\")\nprint(\"=\"*80)\n\n# Run adaptive clustering with Groq model\n(representative_frames, cluster_assignments, \n frame_relevance, high_relevance_count, all_results) = adaptive_clustering_with_groq(\n    frame_feats, config, model, prompter, item, video_captions\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ“Š FINAL RESULTS\")\nprint(\"=\"*80)\n\n# Create comprehensive result\nresult = {\n    'video_id': config.video_id,\n    'feature_file_path': config.feature_file_path,\n    'total_frames': len(frame_feats),\n    'feature_dimensions': frame_feats.shape[1] if len(frame_feats.shape) > 1 else 1,\n    'final_result': {\n        'representative_frames': representative_frames,\n        'cluster_assignments': cluster_assignments,\n        'frame_relevance': frame_relevance,\n        'high_relevance_count': high_relevance_count,\n        'num_clusters': len(set(cluster_assignments)) if cluster_assignments else 0,\n        'passed_threshold': high_relevance_count >= config.iter_threshold\n    },\n    'all_clustering_attempts': all_results,\n    'config': {k: v for k, v in vars(config).items() if not k.startswith('_')},\n    'item_data': item,\n    'model_info': {\n        'model_name': config.model,\n        'total_tokens_used': sum(attempt.get('tokens_used', 0) for attempt in all_results)\n    }\n}\n\n# Print summary\nprint(f\"ğŸ¬ Video ID: {result['video_id']}\")\nprint(f\"ğŸ“Š Total frames: {result['total_frames']}\")\nprint(f\"ğŸ”¢ Feature dimensions: {result['feature_dimensions']}\")\n\nfinal = result['final_result']\nprint(f\"ğŸ¯ Final clusters: {final['num_clusters']}\")\nprint(f\"ğŸ“ Representative frames: {len(final['representative_frames'])}\")\nprint(f\"ğŸ“‹ Frame indices: {final['representative_frames']}\")\nprint(f\"â­ High relevance count: {final['high_relevance_count']}\")\nprint(f\"âœ… Passed threshold ({config.iter_threshold}): {final['passed_threshold']}\")\nprint(f\"ğŸ“ˆ Frame relevance scores: {final['frame_relevance']}\")\n\n# Show clustering progression\nprint(f\"\\nğŸ“ˆ Clustering progression:\")\nfor i, attempt in enumerate(result['all_clustering_attempts']):\n    status = \"ğŸ† FINAL\" if i == len(result['all_clustering_attempts']) - 1 else \"â¡ï¸  continue\"\n    tokens = attempt.get('tokens_used', 0)\n    print(f\"  Attempt {i+1}: {attempt['num_clusters']} clusters â†’ \"\n          f\"{attempt['high_relevance_count']} high-relevance frames ({tokens} tokens) {status}\")\n\ntotal_tokens = result['model_info']['total_tokens_used']\nprint(f\"\\nğŸ”¤ Total tokens used: {total_tokens}\")\n\nprint(f\"\\nâœ… Pipeline completed successfully!\")\nprint(\"=\"*80)\n\n# Cell 8: Save Results\n\n# Create output directory\nmakedir(config.output_base_path)\noutput_path = os.path.join(config.output_base_path, config.output_filename)\n\n# Save results\nsave_json(result, output_path)\n\nprint(\"ğŸ’¾ SAVING RESULTS\")\nprint(\"=\"*50)\nprint(f\"ğŸ“ Output directory: {config.output_base_path}\")\nprint(f\"ğŸ“„ Output file: {config.output_filename}\")\nprint(f\"ğŸ”— Full path: {output_path}\")\n\n# Show file size\nif os.path.exists(output_path):\n    file_size = os.path.getsize(output_path) / 1024  # KB\n    print(f\"ğŸ“ File size: {file_size:.2f} KB\")\n\nprint(f\"\\nâœ… Results saved successfully!\")\n\n# Optional: Show a snippet of the saved file\nprint(f\"\\nğŸ“‹ Results summary:\")\nprint(f\"  - Video: {result['video_id']}\")\nprint(f\"  - Frames: {result['total_frames']}\")\nprint(f\"  - Clusters: {result['final_result']['num_clusters']}\")\nprint(f\"  - Representative frames: {len(result['final_result']['representative_frames'])}\")\nprint(f\"  - High relevance: {result['final_result']['high_relevance_count']}\")\nprint(f\"  - Success: {result['final_result']['passed_threshold']}\")\n\nprint(\"=\"*50)\nprint(\"ğŸ‰ ALL DONE! Your video has been processed with Groq + Llama 3.3 70B!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Depth Expansion","metadata":{}},{"cell_type":"code","source":"# Cell 1: Depth Expansion - Imports and Functions\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom scipy.cluster.hierarchy import linkage, fcluster\nimport json\nimport os\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef hierarchical_clustering_with_external_primary(video_features, cluster_ids, relevance_scores, num_subclusters=5, num_subsubclusters=5):\n    \"\"\"\n    Perform hierarchical clustering based on relevance scores:\n    - Score 1: Keep only primary cluster\n    - Score 2: Split into subclusters  \n    - Score 3: Split into sub-subclusters\n    \"\"\"\n    clusters = {i: {} for i in range(0, max(cluster_ids)+1)}\n\n    for cluster_id in set(cluster_ids):\n        primary_indices = [i for i, x in enumerate(cluster_ids) if x == cluster_id]\n\n        if cluster_id < len(relevance_scores):\n            score = relevance_scores[cluster_id]\n        else:\n            score = 3\n\n        if len(primary_indices) < 2:\n            clusters[cluster_id] = primary_indices\n            continue\n\n        sub_features = video_features[primary_indices]\n\n        if score == 1:\n            # Low relevance: keep as single cluster\n            clusters[cluster_id] = primary_indices\n            continue\n\n        # Create subclusters\n        linked_sub = linkage(sub_features, method='ward')\n        sub_cluster_labels = fcluster(linked_sub, num_subclusters, criterion='maxclust')\n        sub_cluster_labels = sub_cluster_labels - 1\n\n        if score == 2:\n            # Medium relevance: split into subclusters\n            clusters[cluster_id] = {i: [primary_indices[j] for j in np.where(sub_cluster_labels == i)[0]] for i in range(0, num_subclusters)}\n            continue\n\n        # High relevance (score == 3): split into sub-subclusters\n        clusters[cluster_id] = {}\n        for subcluster_id in range(0, num_subclusters):\n            sub_indices = np.where(sub_cluster_labels == subcluster_id)[0]\n            if len(sub_indices) < 2:\n                continue\n\n            subsub_features = sub_features[sub_indices]\n            linked_subsub = linkage(subsub_features, method='ward')\n            subsub_cluster_labels = fcluster(linked_subsub, num_subsubclusters, criterion='maxclust')\n            subsub_cluster_labels = subsub_cluster_labels - 1\n\n            clusters[cluster_id][subcluster_id] = {}\n            for subsubcluster_id in range(0, num_subsubclusters):\n                final_indices = sub_indices[np.where(subsub_cluster_labels == subsubcluster_id)[0]]\n                original_indices = [primary_indices[i] for i in final_indices]\n                clusters[cluster_id][subcluster_id][subsubcluster_id] = original_indices\n\n    return clusters\n\ndef cosine_similarity(points, centroid):\n    \"\"\"Calculate cosine similarity between points and centroid.\"\"\"\n    points_normalized = F.normalize(points, dim=1)\n    centroid_normalized = F.normalize(centroid.unsqueeze(0), dim=1)\n    return 1 - torch.mm(points_normalized, centroid_normalized.T).squeeze()\n\ndef find_closest_points_in_temporal_order_subsub(x, clusters, relevance_scores):\n    \"\"\"Find representative frames from hierarchical clusters in temporal order.\"\"\"\n    closest_points_indices = []\n\n    for cluster_id, cluster_data in clusters.items():\n        if cluster_id < len(relevance_scores):\n            relevance = relevance_scores[cluster_id]\n        else:\n            relevance = 3\n\n        if isinstance(cluster_data, list):  # Primary cluster directly\n            cluster_data = np.array(cluster_data)\n            if cluster_data.size == 0:\n                continue\n            points_in_cluster = x[torch.tensor(cluster_data, dtype=torch.long)]\n            cluster_centroid = points_in_cluster.mean(dim=0)\n            distances = cosine_similarity(points_in_cluster, cluster_centroid)\n            if distances.numel() > 0:\n                closest_idx = torch.argmin(distances).item()\n                closest_points_indices.append(int(cluster_data[closest_idx]))\n\n        elif isinstance(cluster_data, dict):  # Handle subclusters and sub-subclusters\n            if relevance == 1:\n                # Only take representative frame for primary cluster\n                primary_indices = []\n                for subcluster_data in cluster_data.values():\n                    if isinstance(subcluster_data, dict):\n                        for sub_data in subcluster_data.values():\n                            if len(sub_data) > 0:\n                                primary_indices.extend(sub_data)\n                    elif isinstance(subcluster_data, list) and len(subcluster_data) > 0:\n                        primary_indices.extend(subcluster_data)\n\n                if primary_indices:\n                    primary_indices = np.array(primary_indices)\n                    primary_points = x[torch.tensor(primary_indices, dtype=torch.long)]\n                    primary_centroid = primary_points.mean(dim=0)\n                    primary_distances = cosine_similarity(primary_points, primary_centroid)\n                    if primary_distances.numel() > 0:\n                        closest_primary_idx = torch.argmin(primary_distances).item()\n                        closest_points_indices.append(int(primary_indices[closest_primary_idx]))\n                continue\n\n            elif relevance == 2 or relevance == 3:\n                # Include primary cluster representative\n                primary_indices = []\n                for subcluster_data in cluster_data.values():\n                    if isinstance(subcluster_data, dict):\n                        for sub_data in subcluster_data.values():\n                            if len(sub_data) > 0:\n                                primary_indices.extend(sub_data)\n                    elif isinstance(subcluster_data, list) and len(subcluster_data) > 0:\n                        primary_indices.extend(subcluster_data)\n\n                if primary_indices:\n                    primary_indices = np.array(primary_indices)\n                    primary_points = x[torch.tensor(primary_indices, dtype=torch.long)]\n                    primary_centroid = primary_points.mean(dim=0)\n                    primary_distances = cosine_similarity(primary_points, primary_centroid)\n                    if primary_distances.numel() > 0:\n                        closest_primary_idx = torch.argmin(primary_distances).item()\n                        closest_points_indices.append(int(primary_indices[closest_primary_idx]))\n\n                # Process subclusters/sub-subclusters\n                for subcluster_id, subclusters in cluster_data.items():\n                    if isinstance(subclusters, dict):  # Sub-subclusters\n                        for subsubcluster_id, indices in subclusters.items():\n                            if len(indices) == 0:\n                                continue\n                            indices_tensor = torch.tensor(indices, dtype=torch.long)\n                            points_in_subsubcluster = x[indices_tensor]\n                            subsubcluster_centroid = points_in_subsubcluster.mean(dim=0)\n                            distances = cosine_similarity(points_in_subsubcluster, subsubcluster_centroid)\n                            if distances.numel() > 0:\n                                closest_idx_in_subsubcluster = torch.argmin(distances).item()\n                                closest_global_idx = indices[closest_idx_in_subsubcluster]\n                                closest_points_indices.append(int(closest_global_idx))\n\n                    elif isinstance(subclusters, list):\n                        subclusters = np.array(subclusters)\n                        if subclusters.size == 0:\n                            continue\n                        points_in_subcluster = x[torch.tensor(subclusters, dtype=torch.long)]\n                        subcluster_centroid = points_in_subcluster.mean(dim=0)\n                        distances = cosine_similarity(points_in_subcluster, subcluster_centroid)\n                        if distances.numel() > 0:\n                            closest_idx = torch.argmin(distances).item()\n                            closest_points_indices.append(int(subclusters[closest_idx]))\n\n    closest_points_indices.sort()  # Ensure temporal order\n    return closest_points_indices\n\ndef load_image_features(name_ids, save_folder):\n    \"\"\"Load image features from a .pt file.\"\"\"\n    filename = f\"{name_ids}.pt\"\n    filepath = os.path.join(save_folder, filename)\n    img_feats = torch.load(filepath)\n    return img_feats\n\ndef load_json(fn):\n    \"\"\"Load JSON file.\"\"\"\n    with open(fn, 'r') as f:\n        data = json.load(f)\n    return data\n\ndef save_json(data, fn, indent=4):\n    \"\"\"Save data to JSON file.\"\"\"\n    with open(fn, 'w') as f:\n        json.dump(data, f, indent=indent)\n\nprint(\"âœ… Depth expansion functions loaded!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Load Previous Results and Setup Paths\n\n# Configuration for depth expansion\nclass DepthExpansionConfig:\n    def __init__(self):\n        # Input paths - UPDATE THESE\n        self.save_folder = '/kaggle/working/extracted_features'  # Where your .pt files are\n        self.video_id = 'fed08b9b-7cbf-4f96-86a0-567a96b80125'  # Your video ID\n        \n        # Previous results from Groq pipeline\n        self.groq_results_path = './outputs/single_video_groq_pipeline.json'\n        \n        # Output paths\n        self.output_base_path = './outputs'\n        self.output_filename = 'depth_expansion_results.json'\n        \n        # Hierarchical clustering parameters\n        self.num_subclusters = 4\n        self.num_subsubclusters = 4\n\nconfig = DepthExpansionConfig()\nprint(\"ğŸ“‹ Depth Expansion Configuration:\")\nprint(f\"  - Video ID: {config.video_id}\")\nprint(f\"  - Features folder: {config.save_folder}\")\nprint(f\"  - Groq results: {config.groq_results_path}\")\nprint(f\"  - Output: {config.output_filename}\")\n\n# Load previous Groq pipeline results\nprint(f\"\\nğŸ“‚ Loading previous Groq results...\")\ntry:\n    with open(config.groq_results_path, 'r') as f:\n        groq_results = json.load(f)\n    \n    print(\"âœ… Groq results loaded successfully!\")\n    \n    # Extract needed data\n    cluster_assignments = groq_results['final_result']['cluster_assignments']\n    frame_relevance = groq_results['final_result']['frame_relevance'] \n    representative_frames = groq_results['final_result']['representative_frames']\n    \n    print(f\"ğŸ“Š Data extracted:\")\n    print(f\"  - Total frames: {groq_results['total_frames']}\")\n    print(f\"  - Clusters: {len(set(cluster_assignments))}\")\n    print(f\"  - Representative frames: {len(representative_frames)}\")\n    print(f\"  - Frame relevance: {frame_relevance}\")\n    print(f\"  - Cluster assignments: {cluster_assignments[:10]}...\")  # Show first 10\n    \nexcept FileNotFoundError:\n    print(f\"âŒ Error: Could not find Groq results at {config.groq_results_path}\")\n    print(\"Please run the Groq pipeline first!\")\nexcept Exception as e:\n    print(f\"âŒ Error loading Groq results: {e}\")\n\n# Check if feature file exists\nfeature_path = os.path.join(config.save_folder, f\"{config.video_id}.pt\")\nif os.path.exists(feature_path):\n    print(f\"âœ… Feature file found: {feature_path}\")\nelse:\n    print(f\"âŒ Feature file not found: {feature_path}\")\n\nprint(\"\\n\" + \"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Run Depth Expansion\n\nprint(\"ğŸš€ STARTING DEPTH EXPANSION\")\nprint(\"=\"*60)\n\n# Load video features\nprint(f\"ğŸ“‚ Loading features for {config.video_id}...\")\nimg_feats = load_image_features(config.video_id, config.save_folder)\nimg_feats = img_feats.cpu()  # Move to CPU for scipy operations\nprint(f\"âœ… Features loaded: shape {img_feats.shape}\")\n\n# Use frame relevance as cluster relevance scores\n# Map frame relevance to cluster relevance\nprint(f\"ğŸ“Š Processing relevance scores...\")\n\n# Check if we have frame relevance scores\nif isinstance(frame_relevance, list) and len(frame_relevance) > 0:\n    print(f\"âœ… Using extracted relevance scores: {frame_relevance}\")\n    relevance_scores = frame_relevance\nelse:\n    print(\"âš ï¸  No relevance scores found, using default scores...\")\n    # Create default relevance scores (all medium relevance)\n    num_clusters = len(set(cluster_assignments))\n    relevance_scores = [2] * num_clusters  # Default to medium relevance\n\nprint(f\"ğŸ“ˆ Relevance scores: {relevance_scores}\")\nprint(f\"ğŸ¯ Cluster assignments: {len(cluster_assignments)} total assignments\")\n\n# Perform hierarchical clustering with external primary clusters\nprint(f\"\\nğŸ”„ Performing hierarchical clustering...\")\nprint(f\"  - Primary clusters: {len(set(cluster_assignments))}\")\nprint(f\"  - Subclusters per cluster: {config.num_subclusters}\")\nprint(f\"  - Sub-subclusters per subcluster: {config.num_subsubclusters}\")\n\nclusters_info = hierarchical_clustering_with_external_primary(\n    img_feats, \n    cluster_assignments, \n    relevance_scores,\n    num_subclusters=config.num_subclusters,\n    num_subsubclusters=config.num_subsubclusters\n)\n\nprint(f\"âœ… Hierarchical clustering complete!\")\nprint(f\"ğŸ“Š Clusters info type: {type(clusters_info)}\")\n\n# Find representative points in temporal order\nprint(f\"\\nğŸ¯ Finding representative points in temporal order...\")\nclosest_points_temporal = find_closest_points_in_temporal_order_subsub(\n    img_feats, \n    clusters_info, \n    relevance_scores\n)\n\nprint(f\"âœ… Representative points found!\")\nprint(f\"ğŸ“ Number of representative points: {len(closest_points_temporal)}\")\nprint(f\"ğŸ• Temporal order: {closest_points_temporal}\")\n\n# Compare with original representative frames\nprint(f\"\\nğŸ“Š COMPARISON:\")\nprint(f\"  Original (width expansion): {representative_frames}\")\nprint(f\"  New (depth expansion): {closest_points_temporal}\")\nprint(f\"  Original count: {len(representative_frames)}\")\nprint(f\"  New count: {len(closest_points_temporal)}\")\n\n# Calculate expansion ratio\nif len(representative_frames) > 0:\n    expansion_ratio = len(closest_points_temporal) / len(representative_frames)\n    print(f\"  ğŸ“ˆ Expansion ratio: {expansion_ratio:.2f}x\")\nelse:\n    expansion_ratio = 0\n    print(f\"  âŒ No original frames to compare\")\n\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Save Results and Analysis\n\n# Create comprehensive results\ndepth_results = {\n    \"video_id\": config.video_id,\n    \"input_data\": {\n        \"total_frames\": len(img_feats),\n        \"feature_dimensions\": img_feats.shape[1] if len(img_feats.shape) > 1 else 1,\n        \"original_clusters\": len(set(cluster_assignments)),\n        \"original_representative_frames\": representative_frames,\n        \"relevance_scores\": relevance_scores,\n        \"cluster_assignments\": cluster_assignments\n    },\n    \"depth_expansion_config\": {\n        \"num_subclusters\": config.num_subclusters,\n        \"num_subsubclusters\": config.num_subsubclusters\n    },\n    \"results\": {\n        \"hierarchical_clusters\": len(clusters_info),\n        \"final_representative_frames\": closest_points_temporal,\n        \"expansion_ratio\": len(closest_points_temporal) / len(representative_frames) if len(representative_frames) > 0 else 0,\n        \"total_representative_frames\": len(closest_points_temporal)\n    },\n    \"analysis\": {\n        \"relevance_distribution\": {\n            \"score_1\": relevance_scores.count(1) if isinstance(relevance_scores, list) else 0,\n            \"score_2\": relevance_scores.count(2) if isinstance(relevance_scores, list) else 0,\n            \"score_3\": relevance_scores.count(3) if isinstance(relevance_scores, list) else 0\n        },\n        \"frame_coverage\": {\n            \"min_frame\": min(closest_points_temporal) if closest_points_temporal else 0,\n            \"max_frame\": max(closest_points_temporal) if closest_points_temporal else 0,\n            \"frame_span\": max(closest_points_temporal) - min(closest_points_temporal) + 1 if closest_points_temporal else 0\n        }\n    }\n}\n\n# Save results\noutput_path = os.path.join(config.output_base_path, config.output_filename)\nsave_json(depth_results, output_path)\n\nprint(\"ğŸ’¾ SAVING DEPTH EXPANSION RESULTS\")\nprint(\"=\"*50)\nprint(f\"ğŸ“ Output path: {output_path}\")\n\n# Show file size\nif os.path.exists(output_path):\n    file_size = os.path.getsize(output_path) / 1024  # KB\n    print(f\"ğŸ“ File size: {file_size:.2f} KB\")\n\nprint(f\"âœ… Results saved successfully!\")\n\n# Print detailed analysis\nprint(f\"\\nğŸ“Š DEPTH EXPANSION ANALYSIS\")\nprint(\"=\"*50)\n\nresults = depth_results[\"results\"]\nanalysis = depth_results[\"analysis\"]\n\nprint(f\"ğŸ¬ Video: {depth_results['video_id']}\")\nprint(f\"ğŸ“Š Total frames: {depth_results['input_data']['total_frames']}\")\nprint(f\"ğŸ”¢ Feature dimensions: {depth_results['input_data']['feature_dimensions']}\")\n\nprint(f\"\\nğŸ“ˆ Expansion Results:\")\nprint(f\"  - Original clusters: {depth_results['input_data']['original_clusters']}\")\nprint(f\"  - Original representative frames: {len(depth_results['input_data']['original_representative_frames'])}\")\nprint(f\"  - New representative frames: {results['total_representative_frames']}\")\nprint(f\"  - Expansion ratio: {results['expansion_ratio']:.2f}x\")\n\nprint(f\"\\nâ­ Relevance Distribution:\")\nrel_dist = analysis[\"relevance_distribution\"]\nprint(f\"  - Score 1 (Low): {rel_dist['score_1']} clusters\")\nprint(f\"  - Score 2 (Medium): {rel_dist['score_2']} clusters\")\nprint(f\"  - Score 3 (High): {rel_dist['score_3']} clusters\")\n\nprint(f\"\\nğŸ¯ Frame Coverage:\")\ncoverage = analysis[\"frame_coverage\"]\nprint(f\"  - Frame range: {coverage['min_frame']} â†’ {coverage['max_frame']}\")\nprint(f\"  - Frame span: {coverage['frame_span']} frames\")\n\nprint(f\"\\nğŸ“ Representative Frames:\")\nprint(f\"  Original: {depth_results['input_data']['original_representative_frames']}\")\nprint(f\"  Expanded: {results['final_representative_frames']}\")\n\nprint(\"=\"*50)\nprint(\"ğŸ‰ DEPTH EXPANSION COMPLETE!\")\n\n# Quick summary\nprint(f\"\\nğŸ“‹ SUMMARY:\")\nif results['expansion_ratio'] > 1:\n    print(f\"âœ… Successfully expanded from {len(representative_frames)} to {results['total_representative_frames']} frames\")\n    print(f\"ğŸ“ˆ {results['expansion_ratio']:.1f}x more detailed frame selection!\")\nelif results['expansion_ratio'] == 1:\n    print(f\"â¡ï¸  Same number of frames, but hierarchically organized\")\nelse:\n    print(f\"ğŸ“‰ Fewer frames selected: {results['total_representative_frames']} vs {len(representative_frames)}\")\n\nprint(f\"ğŸ¬ Your video now has {results['total_representative_frames']} key representative frames!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LLM Reasoning","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport base64\nimport cv2\nfrom groq import Groq\n\nclass SimpleVideoQA:\n    def __init__(self):\n        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n        self.vision_model = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n        self.text_model = \"llama-3.3-70b-versatile\"\n    \n    def load_frames(self):\n        \"\"\"Load the frames from depth expansion results\"\"\"\n        with open('/kaggle/working/VideoTree/outputs/depth_expansion_results.json', 'r') as f:\n            data = json.load(f)\n        \n        frames = data['results']['final_representative_frames']\n        video_id = data['video_id']\n        \n        print(f\"ğŸ“ Loaded {len(frames)} frames for video {video_id}\")\n        print(f\"ğŸ“Š Frame range: {min(frames)} â†’ {max(frames)}\")\n        return frames, video_id\n    \n    def extract_frame(self, video_path, frame_num):\n        \"\"\"Extract one frame from video\"\"\"\n        cap = cv2.VideoCapture(video_path)\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n        ret, frame = cap.read()\n        cap.release()\n        \n        if ret:\n            temp_path = f\"./temp_frame_{frame_num}.jpg\"\n            cv2.imwrite(temp_path, frame)\n            return temp_path\n        return None\n    \n    def caption_frame(self, frame_path, frame_num):\n        \"\"\"Caption one frame with Groq vision\"\"\"\n        try:\n            # Encode image\n            with open(frame_path, \"rb\") as f:\n                base64_image = base64.b64encode(f.read()).decode('utf-8')\n            \n            # Get caption\n            response = self.client.chat.completions.create(\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"Describe what the person is doing in this frame.\"},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n                    ]\n                }],\n                model=self.vision_model,\n                max_tokens=100\n            )\n            \n            caption = response.choices[0].message.content.strip()\n            \n            # Cleanup\n            os.remove(frame_path)\n            \n            return caption\n            \n        except Exception as e:\n            print(f\"Error captioning frame {frame_num}: {e}\")\n            if os.path.exists(frame_path):\n                os.remove(frame_path)\n            return f\"Frame {frame_num}: [Caption failed]\"\n    \n    def caption_all_frames(self, frames, video_path):\n        \"\"\"Caption all frames\"\"\"\n        print(f\"ğŸ¬ Captioning {len(frames)} frames...\")\n        \n        captions = []\n        for i, frame_num in enumerate(frames):\n            print(f\"  Frame {i+1}/{len(frames)} (frame {frame_num})\", end='\\r')\n            \n            # Extract frame\n            frame_path = self.extract_frame(video_path, frame_num)\n            \n            if frame_path:\n                # Caption frame\n                caption = self.caption_frame(frame_path, frame_num)\n                captions.append(f\"Frame {frame_num}: {caption}\")\n            else:\n                captions.append(f\"Frame {frame_num}: [Extraction failed]\")\n        \n        print(f\"\\nâœ… Captioned {len(captions)} frames\")\n        return captions\n    \n    def answer_question(self, captions, question):\n        \"\"\"Send captions + question to LLM, get answer\"\"\"\n        print(f\"ğŸ¤– Answering question...\")\n        \n        # Join all captions\n        all_captions = \"\\n\".join(captions)\n        \n        # Create prompt\n        prompt = f\"\"\"Here are descriptions of frames from a video:\n\n{all_captions}\n\nQuestion: {question}\n\nAnswer the question based on what you see in these frame descriptions.\"\"\"\n        \n        # Get answer\n        response = self.client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are analyzing video frames to answer questions.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            model=self.text_model,\n            max_tokens=300\n        )\n        \n        answer = response.choices[0].message.content.strip()\n        tokens = response.usage.total_tokens if hasattr(response, 'usage') else 0\n        \n        print(f\"âœ… Answer generated ({tokens} tokens)\")\n        return answer, tokens\n    \n    def save_results(self, frames, captions, question, answer, tokens):\n        \"\"\"Save everything\"\"\"\n        results = {\n            'total_frames': len(frames),\n            'frame_numbers': frames,\n            'captions': captions,\n            'question': question,\n            'answer': answer,\n            'tokens_used': tokens\n        }\n        \n        os.makedirs('/kaggle/working/VideoTree/outputs', exist_ok=True)\n        with open('/kaggle/working/VideoTree/outputs/simple_qa_results.json', 'w') as f:\n            json.dump(results, f, indent=2)\n        \n        print(\"ğŸ’¾ Results saved to /kaggle/working/VideoTree/outputs/simple_qa_results.json\")\n\ndef run_simple_qa(video_path, question):\n    \"\"\"Run the simple caption + QA pipeline\"\"\"\n    \n    print(\"ğŸš€ SIMPLE VIDEO QA PIPELINE\")\n    print(\"=\"*40)\n    \n    qa = SimpleVideoQA()\n    \n    # 1. Load frames\n    frames, video_id = qa.load_frames()\n    \n    # 2. Caption all frames\n    captions = qa.caption_all_frames(frames, video_path)\n    \n    # 3. Answer question\n    answer, tokens = qa.answer_question(captions, question)\n    \n    # 4. Save results\n    qa.save_results(frames, captions, question, answer, tokens)\n    \n    # 5. Show results\n    print(f\"\\nğŸ‰ DONE!\")\n    print(\"=\"*40)\n    print(f\"ğŸ“Š Frames: {len(frames)}\")\n    print(f\"â“ Question: {question}\")\n    print(f\"ğŸ’¬ Answer: {answer}\")\n    print(f\"ğŸ”¤ Tokens: {tokens}\")\n\n# Usage\nif __name__ == \"__main__\":\n    video_path = \"/kaggle/input/egoschema/fed08b9b-7cbf-4f96-86a0-567a96b80125.mp4\"\n    question = \"What is the person doing in this video?\"\n    \n    if os.path.exists(video_path):\n        run_simple_qa(video_path, question)\n    else:\n        print(f\"Video not found: {video_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Uniform Sampling - 85 frames","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nimport base64\nimport cv2\nimport numpy as np\nfrom groq import Groq\n\nclass UniformSamplingQA:\n    def __init__(self):\n        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n        self.vision_model = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n        self.text_model = \"llama-3.3-70b-versatile\"\n    \n    def get_uniform_frames(self, total_frames, num_samples=85):\n        \"\"\"Generate 85 uniformly sampled frame indices\"\"\"\n        # Create uniform spacing\n        if num_samples >= total_frames:\n            frames = list(range(total_frames))\n        else:\n            # Calculate step size for uniform sampling\n            step = total_frames / num_samples\n            frames = [int(i * step) for i in range(num_samples)]\n            \n            # Ensure we don't exceed total frames\n            frames = [min(f, total_frames - 1) for f in frames]\n        \n        print(f\"ğŸ“ Generated {len(frames)} uniform frames from {total_frames} total frames\")\n        print(f\"ğŸ“Š Frame range: {min(frames)} â†’ {max(frames)}\")\n        print(f\"ğŸ“ Step size: ~{step:.2f}\")\n        \n        return frames\n    \n    def extract_frame(self, video_path, frame_num):\n        \"\"\"Extract one frame from video\"\"\"\n        cap = cv2.VideoCapture(video_path)\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n        ret, frame = cap.read()\n        cap.release()\n        \n        if ret:\n            temp_path = f\"./temp_uniform_frame_{frame_num}.jpg\"\n            cv2.imwrite(temp_path, frame)\n            return temp_path\n        return None\n    \n    def caption_frame(self, frame_path, frame_num):\n        \"\"\"Caption one frame with Groq vision\"\"\"\n        try:\n            # Encode image\n            with open(frame_path, \"rb\") as f:\n                base64_image = base64.b64encode(f.read()).decode('utf-8')\n            \n            # Get caption\n            response = self.client.chat.completions.create(\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"Describe what the person is doing in this frame.\"},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n                    ]\n                }],\n                model=self.vision_model,\n                max_tokens=100\n            )\n            \n            caption = response.choices[0].message.content.strip()\n            \n            # Cleanup\n            os.remove(frame_path)\n            \n            return caption\n            \n        except Exception as e:\n            print(f\"Error captioning frame {frame_num}: {e}\")\n            if os.path.exists(frame_path):\n                os.remove(frame_path)\n            return f\"Frame {frame_num}: [Caption failed]\"\n    \n    def caption_all_frames(self, frames, video_path):\n        \"\"\"Caption all frames\"\"\"\n        print(f\"ğŸ¬ Captioning {len(frames)} uniformly sampled frames...\")\n        \n        captions = []\n        for i, frame_num in enumerate(frames):\n            print(f\"  Frame {i+1}/{len(frames)} (frame {frame_num})\", end='\\r')\n            \n            # Extract frame\n            frame_path = self.extract_frame(video_path, frame_num)\n            \n            if frame_path:\n                # Caption frame\n                caption = self.caption_frame(frame_path, frame_num)\n                captions.append(f\"Frame {frame_num}: {caption}\")\n            else:\n                captions.append(f\"Frame {frame_num}: [Extraction failed]\")\n        \n        print(f\"\\nâœ… Captioned {len(captions)} frames\")\n        return captions\n    \n    def answer_question(self, captions, question):\n        \"\"\"Send captions + question to LLM, get answer\"\"\"\n        print(f\"ğŸ¤– Answering question...\")\n        \n        # Join all captions\n        all_captions = \"\\n\".join(captions)\n        \n        # Create prompt\n        prompt = f\"\"\"Here are descriptions of uniformly sampled frames from a video:\n\n{all_captions}\n\nQuestion: {question}\n\nAnswer the question based on what you see in these frame descriptions.\"\"\"\n        \n        # Get answer\n        response = self.client.chat.completions.create(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are analyzing uniformly sampled video frames to answer questions.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            model=self.text_model,\n            max_tokens=300\n        )\n        \n        answer = response.choices[0].message.content.strip()\n        tokens = response.usage.total_tokens if hasattr(response, 'usage') else 0\n        \n        print(f\"âœ… Answer generated ({tokens} tokens)\")\n        return answer, tokens\n    \n    def save_results(self, frames, captions, question, answer, tokens, total_frames):\n        \"\"\"Save everything\"\"\"\n        results = {\n            'sampling_method': 'uniform',\n            'total_video_frames': total_frames,\n            'sampled_frames': len(frames),\n            'frame_numbers': frames,\n            'captions': captions,\n            'question': question,\n            'answer': answer,\n            'tokens_used': tokens,\n            'sampling_info': {\n                'step_size': total_frames / len(frames),\n                'coverage': f\"{min(frames)}-{max(frames)}\"\n            }\n        }\n        \n        os.makedirs('/kaggle/working/VideoTree/outputs', exist_ok=True)\n        with open('/kaggle/working/VideoTree/outputs/uniform_sampling_qa_results.json', 'w') as f:\n            json.dump(results, f, indent=2)\n        \n        print(\"ğŸ’¾ Results saved to /kaggle/working/VideoTree/outputs/uniform_sampling_qa_results.json\")\n\ndef run_uniform_sampling_qa(video_path, question, total_frames=180, num_samples=85):\n    \"\"\"Run the uniform sampling caption + QA pipeline\"\"\"\n    \n    print(\"ğŸš€ UNIFORM SAMPLING VIDEO QA PIPELINE\")\n    print(\"=\"*45)\n    \n    qa = UniformSamplingQA()\n    \n    # 1. Generate uniform frame indices\n    frames = qa.get_uniform_frames(total_frames, num_samples)\n    \n    # 2. Caption all frames\n    captions = qa.caption_all_frames(frames, video_path)\n    \n    # 3. Answer question\n    answer, tokens = qa.answer_question(captions, question)\n    \n    # 4. Save results\n    qa.save_results(frames, captions, question, answer, tokens, total_frames)\n    \n    # 5. Show results\n    print(f\"\\nğŸ‰ UNIFORM SAMPLING DONE!\")\n    print(\"=\"*45)\n    print(f\"ğŸ“Š Total frames: {total_frames}\")\n    print(f\"ğŸ“ Sampled frames: {len(frames)}\")\n    print(f\"ğŸ“ Step size: ~{total_frames/len(frames):.2f}\")\n    print(f\"â“ Question: {question}\")\n    print(f\"ğŸ’¬ Answer: {answer}\")\n    print(f\"ğŸ”¤ Tokens: {tokens}\")\n\n# Usage\nif __name__ == \"__main__\":\n    video_path = \"/kaggle/input/egoschema/fed08b9b-7cbf-4f96-86a0-567a96b80125.mp4\"\n    question = \"What is the person doing in this video?\"\n    \n    if os.path.exists(video_path):\n        run_uniform_sampling_qa(video_path, question, total_frames=180, num_samples=85)\n    else:\n        print(f\"Video not found: {video_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
